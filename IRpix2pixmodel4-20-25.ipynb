{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bschulman/Algo-projects/blob/main/IRpix2pixmodel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kJ116tdap9h"
      },
      "source": [
        "## Install, Import, Mount Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "SZ-NYBi9ZQD_"
      },
      "outputs": [],
      "source": [
        "!pip install neptune\n",
        "#!pip install tensorflow-addons\n",
        "!pip install opencv-python\n",
        "!pip install -U tensorboard_plugin_profile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skpVCakaXdyu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import logging\n",
        "from google.colab import drive, userdata\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model, Input, losses, optimizers, mixed_precision, models, applications, callbacks\n",
        "try:\n",
        "    import neptune\n",
        "except ImportError:\n",
        "    print(\"Neptune client not found. Please install with: pip install -U neptune\")\n",
        "    neptune = None\n",
        "import time # For timing epochs\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import SpectralNormalization, BatchNormalization, GroupNormalization\n",
        "from tensorflow.keras.applications import vgg19, VGG16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorboard import notebook\n",
        "import glob\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import pkgutil\n",
        "print(pkgutil.find_loader(\"tensorboard_plugin_profile\") is not None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEgp-kd3Unze"
      },
      "outputs": [],
      "source": [
        "if os.path.exists(\"/content/drive\"):\n",
        "  pass\n",
        "else:\n",
        "  drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ww8bom-zGdCd"
      },
      "source": [
        "## General config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MG90Ttn0STSs"
      },
      "outputs": [],
      "source": [
        "log_dir = './logs/'  # Define the log directory\n",
        "writer = tf.summary.create_file_writer(log_dir)  # Use tf.summary.create_file_writer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "60SGw_t8Q-6l"
      },
      "outputs": [],
      "source": [
        "RUN_NAME = 'pix2pix-baselineD_BCE'\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jo5nw30eV39e"
      },
      "outputs": [],
      "source": [
        "# --- Configuration for Models ---\n",
        "INPUT_CHANNELS = 3 # Generator input channels (e.g., 3 for RGB)\n",
        "OUTPUT_CHANNELS = 3 # Generator output channels (e.g., 3 for RGB)\n",
        "IMG_HEIGHT = 256 # Defined in previous data pipeline setup\n",
        "IMG_WIDTH = 256  # Defined in previous data pipeline setup\n",
        "# Define if input is left or right half\n",
        "INPUT_IS_LEFT_HALF = True # Assume [Photo | Drawing]\n",
        "NORMALIZATION_RANGE = (-1, 1) # Assuming data normalized to [-1, 1]\n",
        "AUGMENTATIONS = 'random_horizontal_flip + random_jitter'\n",
        "TARGET_CHANNELS = 3\n",
        "VAL_SPLIT = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Xq4rQ7-WEIo"
      },
      "outputs": [],
      "source": [
        "GENERATOR_ARCHITECTURE = 'U-Net'\n",
        "DISCRIMINATOR_ARCHITECTURE = 'PatchGAN'\n",
        "GENERATOR_OUTPUT_ACTIVATION = 'tanh' # Assuming tanh for [-1, 1] output range\n",
        "use_batchnorm = False # Set to False if InstanceNorm\n",
        "NORMALIZATION_LAYER = 'BatchNormalization'\n",
        "GENERATOR_FILTERS_INITIAL = 64 # Starting filter count in generator\n",
        "DISCRIMINATOR_FILTERS_INITIAL = 64 # Starting filter count in discriminator\n",
        "BUFFER_SIZE = 400 # For shuffling. Should be >= dataset size for perfect shuffle, but smaller works.\n",
        "BATCH_SIZE = 32 # For Colab Pro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgAe7kD5WLOR"
      },
      "outputs": [],
      "source": [
        "# Define hyperparameters based on common pix2pix settings\n",
        "OPTIMIZER_TYPE = 'Adam'\n",
        "WEIGHT_DECAY = 0.0 # No weight decay in Adam by default\n",
        "LEARNING_RATE = 0.0004\n",
        "DISCRIMINATOR_LR_MULTIPLIER = 0.5\n",
        "BETA_1 = 0.5 # Beta1 commonly set to 0.5 for GANs using Adam for stability\n",
        "BETA_2 = 0.999 # Default beta2 is usually fine\n",
        "DISC_UPDATE = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5GATqgVWNLy"
      },
      "outputs": [],
      "source": [
        "# Loss\n",
        "ADVERSARIAL_LOSS_TYPE = 'BCE_logits'\n",
        "RECONSTRUCTION_LOSS_TYPE = 'perceptual + edge'\n",
        "LAMBDA_L1 = 0\n",
        "LAMBDA_EDGE = 30\n",
        "LAMBDA_P = 0.01\n",
        "LOSS_LAYERS = ['block3_conv3', 'block4_conv3']\n",
        "LOSS_MODEL = VGG16\n",
        "LAYER_WEIGHTS = [1.0,1.0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvKGQ4AgWO1g"
      },
      "outputs": [],
      "source": [
        "#Neptune\n",
        "NEPTUNE_PROJECT = \"IR-MAPPINGS/overviews\"\n",
        "NEPTUNE_API_TOKEN = userdata.get('NEPTUNE_API_TOKEN')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHyRnnlTXIgb"
      },
      "outputs": [],
      "source": [
        "# === PROFILER CONFIG: add at top, right after your imports ===\n",
        "# which epochs to profile, or toggle on/off by creating/removing this file\n",
        "PROFILE_EPOCHS     = {0, 1, 2, 3, 4, 5, 6, 7}\n",
        "PROFILE_FLAG_FILE  = \"/tmp/enable_profiling.flag\"\n",
        "profiling_active   = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zu6I1kB2WP82"
      },
      "outputs": [],
      "source": [
        "#Training\n",
        "TOTAL_EPOCHS = 100\n",
        "LR_SCHEDULE = 'fixed_100_then_linear_decay_100'\n",
        "LR_DECAY = 1\n",
        "LR_EPOCH = 5\n",
        "# Define frequencies for actions within the loop\n",
        "CHECKPOINT_SAVE_FREQ = 10 # Save checkpoint every N epochs\n",
        "IMAGE_LOG_FREQ = 5      # Log validation images to Neptune every N epochs\n",
        "CONSOLE_LOG_FREQ = 50   # Log average losses to console/file every N steps within an epoch\n",
        "DESCRIPTION = f\"Baseline Pix2Pix: {ADVERSARIAL_LOSS_TYPE}+{RECONSTRUCTION_LOSS_TYPE}(L_l1={LAMBDA_L1}), L_p={LAMBDA_P}, L_edge={LAMBDA_EDGE} Adam(LR={LEARNING_RATE}, B1={BETA_1})\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j72N4YfB0ehW"
      },
      "outputs": [],
      "source": [
        "#Metrics\n",
        "METRIC_LAYER_NAMES = ['block2_conv2', 'block3_conv3', 'block4_conv3']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hInuIF-bbAr_"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Use 'mixed_float16' for broad compatibility and speed (T4, V100, A100)\n",
        "policy = mixed_precision.Policy('mixed_bfloat16')\n",
        "tf.keras.mixed_precision.set_global_policy('mixed_bfloat16')\n",
        "bf16 = tf.bfloat16\n",
        "#logger.info(f\"Mixed precision policy set to: {mixed_precision.global_policy().name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnQCyb4xzOK4"
      },
      "outputs": [],
      "source": [
        "vgg = VGG16(include_top=False, weights='imagenet', input_shape=(None, None, 3))\n",
        "vgg.trainable = False\n",
        "for layer in vgg.layers:\n",
        "    layer.trainable = False # Double-check all layers are frozen\n",
        "# Create a new model that outputs features from the selected layers\n",
        "layer_outputs = [vgg.get_layer(name).output for name in LOSS_LAYERS]\n",
        "vgg_loss_model = Model(inputs=vgg.input, outputs=layer_outputs, name='vgg_loss_model')\n",
        "vgg_loss_model.trainable = False # Ensure the combined model is also not trainable\n",
        "print(\"VGG Loss Model built and frozen.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mPiA0tdUpqo"
      },
      "outputs": [],
      "source": [
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "log_dir = f\"logs/gradient_tape/{current_time}\"\n",
        "tb_writer = tf.summary.create_file_writer(log_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VEWlLgWV9Jo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uiqo5pp4awMD"
      },
      "source": [
        "## Logging Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2t_SRrXMZaJx"
      },
      "outputs": [],
      "source": [
        "# --- Configuration ---\n",
        "LOG_LEVEL = logging.INFO # Set minimum level to log (e.g., INFO, DEBUG)\n",
        "LOG_FORMAT = '%(asctime)s - %(levelname)s - [%(name)s] - %(message)s' # Example format\n",
        "LOG_DATEFMT = '%Y-%m-%d %H:%M:%S'\n",
        "\n",
        "# --- Log to a file in Google Drive ---\n",
        "# For persistent logs across sessions or long runs\n",
        "LOG_TO_FILE = True # Set to False to only log to Colab output\n",
        "log_file_path = '/content/drive/MyDrive/IR_Mappings/pix2pix_training.log'\n",
        "\n",
        "# --- Apply Configuration ---\n",
        "log_handlers = [logging.StreamHandler(sys.stdout)] # Log to Colab output (stdout)\n",
        "if LOG_TO_FILE:\n",
        "    # Ensure directory exists if logging to file\n",
        "    os.makedirs(os.path.dirname(log_file_path), exist_ok=True)\n",
        "    log_handlers.append(logging.FileHandler(log_file_path, mode='a')) # 'a' for append\n",
        "\n",
        "# Use force=True if you might re-run this config cell in the same session (Python 3.8+)\n",
        "# Otherwise, basicConfig only works the first time it's called.\n",
        "logging.basicConfig(\n",
        "    level=LOG_LEVEL,\n",
        "    format=LOG_FORMAT,\n",
        "    datefmt=LOG_DATEFMT,\n",
        "    handlers=log_handlers,\n",
        "    force=True # Uncomment if needed for re-running the cell\n",
        ")\n",
        "\n",
        "# --- Get specific logger ---\n",
        "# Using a named logger allows for more granular control\n",
        "logger = logging.getLogger('Pix2PixTrainer')\n",
        "\n",
        "# --- Test Messages ---\n",
        "logger.info(\"Logging configured. Starting training process...\")\n",
        "logger.debug(\"This is a DEBUG message - it will only show if LOG_LEVEL is set to DEBUG.\")\n",
        "logger.info(\"This is an INFO message.\")\n",
        "logger.warning(\"This is a WARNING message.\")\n",
        "logger.error(\"This is an ERROR message.\")\n",
        "\n",
        "# Example of logging an exception\n",
        "try:\n",
        "    x = 1 / 0\n",
        "except ZeroDivisionError as e:\n",
        "    logger.error(f\"An error occurred: {e}\", exc_info=True) # exc_info=True adds traceback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZCiFreZioXc"
      },
      "outputs": [],
      "source": [
        "# --- TEMPORARY DEBUGGING FOR CONSOLE OUTPUT ---\n",
        "LOG_LEVEL = logging.INFO # Use DEBUG to see everything\n",
        "LOG_FORMAT = '{asctime} - {levelname} - {message}'\n",
        "LOG_DATEFMT = '%H:%M:%S'\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=LOG_LEVEL,\n",
        "    format=LOG_FORMAT,\n",
        "    datefmt=LOG_DATEFMT,\n",
        "    handlers=[logging.StreamHandler(sys.stdout)], # ONLY console handler\n",
        "    force=True, # Force reconfiguration\n",
        "    style='{' # Use old-style formatting (optional)\n",
        ")\n",
        "logging.critical(\"--- CONSOLE-ONLY LOGGING TEST ---\")\n",
        "logger = logging.getLogger('Pix2PixTrainer') # Get logger AFTER config\n",
        "logger.info(\"Info message (console-only)\")\n",
        "logger.debug(\"Debug message (console-only)\")\n",
        "# --- END OF TEMPORARY DEBUGGING BLOCK ---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nbbi4wHPirZd"
      },
      "outputs": [],
      "source": [
        "# # Run this in the cell AFTER the simplified basicConfig\n",
        "# logging.info(\"Direct root logger INFO message.\")\n",
        "# logging.critical(\"Direct root logger CRITICAL message.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cM2m-HrUazGe"
      },
      "source": [
        "## Tf/GPU config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmAToOhDVRD2"
      },
      "outputs": [],
      "source": [
        "logging.info(f\"TensorFlow version: {tf.__version__}\")\n",
        "\n",
        "# Check for GPU availability\n",
        "gpu_devices = tf.config.list_physical_devices('GPU')\n",
        "if gpu_devices:\n",
        "    logging.info(f\"GPU detected: {gpu_devices}\")\n",
        "    # Optional: Set memory growth to prevent TF from allocating all GPU memory upfront\n",
        "    try:\n",
        "        for gpu in gpu_devices:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    except RuntimeError as e:\n",
        "        # Memory growth must be set before GPUs have been initialized\n",
        "        logging.error(e)\n",
        "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "    #logging.info(len(gpu_devices), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "    # Usually computation happens on '/GPU:0'\n",
        "else:\n",
        "    logging.info(f\"No GPU found. Using CPU.\")\n",
        "\n",
        "\n",
        "# List physical devices (GPUs and CPUs)\n",
        "physical_devices = tf.config.list_physical_devices()\n",
        "print(\"All Physical Devices:\", physical_devices)\n",
        "\n",
        "# List GPUs specifically\n",
        "gpu_devices = tf.config.list_physical_devices('GPU')\n",
        "if gpu_devices:\n",
        "    print(\"GPUs Available:\", gpu_devices)\n",
        "else:\n",
        "    print(\"No GPUs found. Using CPU.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzUALCPka1UL"
      },
      "source": [
        "## Directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKcVCIk7XrJV"
      },
      "outputs": [],
      "source": [
        "# Define base directory and subdirectories\n",
        "base_data_dir = '/content/drive/MyDrive/IR_Mappings/IR_pix2pix/'\n",
        "concat_image_dir = os.path.join(base_data_dir, 'concatenated_images')\n",
        "CONCAT_IMAGE_DIR = concat_image_dir\n",
        "checkpoint_dir = '/content/drive/MyDrive/IR_Mappings/IR_pix2pix/training_checkpoints'\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "logger.info(f\"Checkpoint directory set to: {checkpoint_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqgROAWoYIsw"
      },
      "source": [
        "## Test Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcboeZGJYIJ_"
      },
      "outputs": [],
      "source": [
        "# --- Load one example concatenated image ---\n",
        "# '''\n",
        "# try:\n",
        "#     # List files to help find an example (optional)\n",
        "#     print(\"Sample concatenated files:\", os.listdir(concat_image_dir)[:5])\n",
        "\n",
        "#     example_concat_name = 'profiles_i1_concatenated.png' # REPLACE with a real concatenated filename\n",
        "#     example_concat_path = os.path.join(concat_image_dir, example_concat_name)\n",
        "\n",
        "#     # Load the concatenated image using PIL\n",
        "#     # Load as RGB first, can handle diagram channels later if needed\n",
        "#     img_concat = Image.open(example_concat_path).convert(\"RGB\")\n",
        "\n",
        "#     # --- !!! Crucial Step: Split the Image !!! ---\n",
        "#     width, height = img_concat.size\n",
        "#     if width == 512 and height == 256:\n",
        "#         # Format is [PHOTO | DRAWING] (Input | Target) ***\n",
        "\n",
        "#         # PIL's crop box is (left, upper, right, lower)\n",
        "#         img_photo = img_concat.crop((0, 0, 256, 256))      # Left half (width 0 to 256)\n",
        "#         img_diagram = img_concat.crop((256, 0, 512, 256))   # Right half (width 256 to 512)\n",
        "\n",
        "#         # Optional: Convert diagram to grayscale now if desired,\n",
        "#         # otherwise handle channels during TF preprocessing\n",
        "#         # img_diagram = img_diagram.convert(\"L\")\n",
        "\n",
        "#     else:\n",
        "#         print(f\"Error: Unexpected image dimensions {img_concat.size}. Expected 256x512.\")\n",
        "#         # Handle error appropriately, maybe skip this file in a real pipeline\n",
        "#         raise ValueError(\"Incorrect image dimensions\")\n",
        "\n",
        "#     # Display the split parts\n",
        "#     fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
        "#     ax[0].imshow(img_photo)\n",
        "#     ax[0].set_title(f\"Input Photo (Split Left)\")\n",
        "#     ax[0].axis('off')\n",
        "\n",
        "#     # Use grayscale cmap if diagram was converted to 'L' mode above\n",
        "#     ax[1].imshow(img_diagram, cmap='gray' if img_diagram.mode == 'L' else None)\n",
        "#     ax[1].set_title(f\"Target Diagram (Split Right)\")\n",
        "#     ax[1].axis('off')\n",
        "#     plt.show()\n",
        "\n",
        "#     print(f\"Photo size: {img_photo.size}, Mode: {img_photo.mode}\")\n",
        "#     print(f\"Diagram size: {img_diagram.size}, Mode: {img_diagram.mode}\")\n",
        "\n",
        "# except FileNotFoundError:\n",
        "#     print(f\"Error: Example file '{example_concat_name}' not found in '{concat_image_dir}'. Please check path and filename.\")\n",
        "# except Exception as e:\n",
        "#     print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-ruffCAamX7"
      },
      "source": [
        "## Set Up Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6G3jnPVbFOp"
      },
      "outputs": [],
      "source": [
        "# --- Define Loading and Preprocessing Function ---\n",
        "def load_image(image_file):\n",
        "    image = tf.io.read_file(image_file)\n",
        "    # Decode. Use decode_png or decode_jpeg depending on your file type.\n",
        "    # decode_image handles both but might be slower. Ensure 3 channels for RGB.\n",
        "    image = tf.image.decode_png(image, channels=3)\n",
        "    # Ensure output type is float32 for calculations\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    return image\n",
        "def random_crop(image, dim):\n",
        "    height, width, _ = dim\n",
        "    x, y = np.random.uniform(low=0,high=int(height-256)), np.random.uniform(low=0,high=int(width-256))\n",
        "    return image[:, int(x):int(x)+256, int(y):int(y)+256]\n",
        "\n",
        "@tf.function\n",
        "def random_jittering(input_image, target_image, height=286, width=286):\n",
        "    # --- Resize using tf.image ---\n",
        "    # Stack images along the batch dimension temporarily to apply resize consistently\n",
        "    # Note: tf.image.resize expects batch dimension or single image.\n",
        "    # If input_image, target_image are single images (H,W,C), add batch dim.\n",
        "    # If they already have batch dim (B,H,W,C), this might need adjustment.\n",
        "    # Assuming single images from context of preprocess_image_train map:\n",
        "    input_image = tf.expand_dims(input_image, axis=0) # Add batch dim: (1, H, W, C)\n",
        "    target_image = tf.expand_dims(target_image, axis=0)\n",
        "\n",
        "    # Resize (choose method like 'nearest' or 'bilinear')\n",
        "    # tf.image.resize works on batches\n",
        "    resized_input = tf.image.resize(input_image, [height, width], method='nearest')\n",
        "    resized_target = tf.image.resize(target_image, [height, width], method='nearest')\n",
        "\n",
        "    # --- Random Crop using tf.image ---\n",
        "    # Stack resized images to apply the *same* random crop\n",
        "    stacked_images = tf.stack([resized_input[0], resized_target[0]], axis=0) # Stack along new axis 0: (2, H', W', C)\n",
        "\n",
        "    # Apply random crop to the stack\n",
        "    # cropped_images shape will be (2, IMG_HEIGHT, IMG_WIDTH, C)\n",
        "    cropped_images = tf.image.random_crop(stacked_images, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n",
        "\n",
        "    # Unstack back into input and target images\n",
        "    input_image_cropped = cropped_images[0]\n",
        "    target_image_cropped = cropped_images[1]\n",
        "\n",
        "    # Set shapes explicitly (good practice after operations that might lose shape info)\n",
        "    input_image_cropped.set_shape([IMG_HEIGHT, IMG_WIDTH, 3])\n",
        "    target_image_cropped.set_shape([IMG_HEIGHT, IMG_WIDTH, 3])\n",
        "\n",
        "    return input_image_cropped, target_image_cropped\n",
        "\n",
        "def preprocess_image_train(image):\n",
        "    # image shape is (256, 512, 3) initially\n",
        "    logger.debug(f\"Preprocessing input image with shape: {tf.shape(image)}\") # Use logger\n",
        "\n",
        "    # --- 1. Split into Input and Real Image FIRST ---\n",
        "    width = tf.shape(image)[1] // 2 # Should be 256\n",
        "    if INPUT_IS_LEFT_HALF:\n",
        "        # Original is [Photo | Drawing]\n",
        "        input_image = image[:, :width, :]  # Input Photo (Left Half)\n",
        "        real_image = image[:, width:, :]   # Target Diagram (Right Half)\n",
        "        logger.debug(\"Split assuming Photo on Left, Drawing on Right.\")\n",
        "    else:\n",
        "        # Original is [Drawing | Photo]\n",
        "        real_image = image[:, :width, :]   # Target Diagram (Left Half)\n",
        "        input_image = image[:, width:, :]  # Input Photo (Right Half)\n",
        "        logger.debug(\"Split assuming Drawing on Left, Photo on Right.\")\n",
        "\n",
        "    # Ensure shapes after split before augmentation\n",
        "    input_image.set_shape([IMG_HEIGHT, IMG_WIDTH, 3])\n",
        "    real_image.set_shape([IMG_HEIGHT, IMG_WIDTH, 3])\n",
        "\n",
        "    # --- 2. Augmentation (Applied Consistently After Splitting) ---\n",
        "    # Decide whether to flip ONCE for the pair\n",
        "    flip_condition = tf.random.uniform(()) > 0.5\n",
        "    logger.debug(f\"Flip condition evaluated: {flip_condition}\") # Log the decision\n",
        "\n",
        "    # Apply the SAME flip condition to both images using tf.cond\n",
        "    # tf.cond(predicate, function_if_true, function_if_false)\n",
        "    input_image = tf.cond(flip_condition,\n",
        "                          lambda: tf.image.flip_left_right(input_image),\n",
        "                          lambda: input_image)\n",
        "    real_image = tf.cond(flip_condition,\n",
        "                         lambda: tf.image.flip_left_right(real_image),\n",
        "                         lambda: real_image)\n",
        "    if flip_condition:\n",
        "        logger.debug(\"Applied flip_left_right to both input and real image.\")\n",
        "\n",
        "    input_image, real_image = random_jittering(input_image, real_image)\n",
        "    # --- 3. Normalization [-1, 1] ---\n",
        "    input_image = (input_image / 127.5) - 1.0\n",
        "    real_image = (real_image / 127.5) - 1.0\n",
        "    logger.debug(\"Normalized both images to [-1, 1].\")\n",
        "\n",
        "    # Final shape check (optional)\n",
        "    # input_image.set_shape([IMG_HEIGHT, IMG_WIDTH, 3]) # Already set after split\n",
        "    # real_image.set_shape([IMG_HEIGHT, IMG_WIDTH, 3])\n",
        "\n",
        "    return input_image, real_image\n",
        "\n",
        "# Define a preprocessing function *without* random augmentation\n",
        "def preprocess_image_val(image):\n",
        "    # Split\n",
        "    width = tf.shape(image)[1] // 2\n",
        "    if INPUT_IS_LEFT_HALF:\n",
        "        input_image = image[:, :width, :]\n",
        "        real_image = image[:, width:, :]\n",
        "    else:\n",
        "        real_image = image[:, :width, :]\n",
        "        input_image = image[:, width:, :]\n",
        "    input_image.set_shape([IMG_HEIGHT, IMG_WIDTH, 3])\n",
        "    real_image.set_shape([IMG_HEIGHT, IMG_WIDTH, 3])\n",
        "    # Normalize ONLY\n",
        "    input_image = (input_image / 127.5) - 1.0\n",
        "    real_image = (real_image / 127.5) - 1.0\n",
        "    return input_image, real_image\n",
        "\n",
        "logger.info(\"Setting up tf.data pipeline...\")\n",
        "\n",
        "# --- Create Dataset from File Paths ---\n",
        "try:\n",
        "    # 1. Get the list of ALL file paths\n",
        "    all_files_pattern = os.path.join(CONCAT_IMAGE_DIR, '*.png') # Or appropriate pattern\n",
        "    all_files = tf.io.gfile.glob(all_files_pattern)\n",
        "    if not all_files:\n",
        "         raise ValueError(f\"No files found matching pattern: {all_files_pattern}\")\n",
        "    dataset_size = len(all_files)\n",
        "    logger.info(f\"Found {dataset_size} total image files.\")\n",
        "\n",
        "    # 2. Shuffle the LIST of file paths\n",
        "    np.random.seed(42) # Optional: for reproducible shuffles/splits\n",
        "    np.random.shuffle(all_files) # Shuffle the Python list in-place\n",
        "    logger.info(\"Shuffled file list.\")\n",
        "\n",
        "    # 3. Calculate split sizes\n",
        "    num_val_samples = int(dataset_size * VAL_SPLIT)\n",
        "    num_train_samples = dataset_size - num_val_samples\n",
        "    logger.info(f\"Splitting into {num_train_samples} training files and {num_val_samples} validation files.\")\n",
        "\n",
        "    # 4. Split the LIST using slicing\n",
        "    train_files = all_files[:num_train_samples]\n",
        "    val_files = all_files[num_train_samples:]\n",
        "\n",
        "    # 5. Create separate tf.data.Dataset objects from the file lists\n",
        "    train_dataset_paths = tf.data.Dataset.from_tensor_slices(train_files)\n",
        "    val_dataset_paths = tf.data.Dataset.from_tensor_slices(val_files)\n",
        "\n",
        "    # --- Now build the actual data pipelines ---\n",
        "    logger.info(\"Building train_dataset pipeline...\")\n",
        "    # Adjust buffer size based on train set size\n",
        "    BUFFER_SIZE = min(BUFFER_SIZE, num_train_samples)\n",
        "\n",
        "    train_dataset = train_dataset_paths.shuffle(BUFFER_SIZE) # Shuffle train data each epoch\n",
        "    train_dataset = train_dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    train_dataset = train_dataset.map(preprocess_image_train, num_parallel_calls=tf.data.AUTOTUNE) # Has augmentation\n",
        "    train_dataset = train_dataset.batch(BATCH_SIZE)\n",
        "    train_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "    logger.info(\"Training dataset pipeline built successfully.\")\n",
        "\n",
        "    logger.info(\"Building val_dataset pipeline...\")\n",
        "    val_dataset = val_dataset_paths.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    # Use a non-augmenting preprocessing function for validation\n",
        "    val_dataset = val_dataset.map(preprocess_image_val, num_parallel_calls=tf.data.AUTOTUNE) # Assumes preprocess_image_val exists\n",
        "    val_dataset = val_dataset.batch(BATCH_SIZE) # Use same batch size\n",
        "    val_dataset = val_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "    logger.info(\"Validation dataset pipeline built successfully.\")\n",
        "\n",
        "    # --- Update: Fetch fixed validation batch from val_dataset ---\n",
        "    logger.info(\"Fetching fixed validation batch for visualization from val_dataset...\")\n",
        "    fixed_val_input, fixed_val_target = None, None\n",
        "    try:\n",
        "        val_examples_to_take = min(BATCH_SIZE, 16)\n",
        "        # Take from the start of the (unshuffled) validation set\n",
        "        fixed_val_batch_dataset = val_dataset.take(1).unbatch().take(val_examples_to_take).batch(val_examples_to_take)\n",
        "        fixed_val_input, fixed_val_target = next(iter(fixed_val_batch_dataset))\n",
        "        logger.info(f\"Fixed validation batch shapes: Input {fixed_val_input.shape}, Target {fixed_val_target.shape}\")\n",
        "    except Exception as e:\n",
        "         logger.error(f\"Could not get validation batch: {e}\", exc_info=True)\n",
        "         logger.warning(\"Proceeding without fixed validation data for image logging.\")\n",
        "    # dataset = tf.data.Dataset.list_files(os.path.join(CONCAT_IMAGE_DIR, '*.png'))\n",
        "    # # Count items (optional but good for sanity check)\n",
        "    # dataset_size = tf.data.experimental.cardinality(dataset).numpy()\n",
        "    # if dataset_size == 0:\n",
        "    #       raise ValueError(f\"No files found matching pattern in {CONCAT_IMAGE_DIR}\")\n",
        "    # logger.info(f\"Found {dataset_size} image files.\")\n",
        "    # num_val_samples = int(dataset_size * VAL_SPLIT)\n",
        "    # num_train_samples = dataset_size - num_val_samples\n",
        "    # np.random.shuffle(dataset)\n",
        "    # train_files = dataset[:num_train_samples]\n",
        "    # val_files = dataset[num_train_samples:]\n",
        "    # logger.info(f\"Split into {len(train_files)} training files and {len(val_files)} validation files.\")\n",
        "    # # Set buffer size based on dataset size if needed\n",
        "    # BUFFER_SIZE = min(BUFFER_SIZE, len(train_files))\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error creating dataset from files: {e}\", exc_info=True)\n",
        "    # Stop execution or handle error\n",
        "    raise\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# val_dataset = tf.data.Dataset.from_tensor_slices(val_files) # Use val_files list\n",
        "# val_dataset = val_dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "# val_dataset = val_dataset.map(preprocess_image_val, num_parallel_calls=tf.data.AUTOTUNE) # Use non-augmenting preprocess\n",
        "# val_dataset = val_dataset.batch(BATCH_SIZE) # Use same batch size for efficiency\n",
        "# val_dataset = val_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "# logger.info(\"Validation dataset pipeline built successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDQrk5DydDra"
      },
      "source": [
        "## Calculate Step-Based Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_glV4tTUdCrk"
      },
      "outputs": [],
      "source": [
        "# === LEARNING RATE SCHEDULE SETUP ===\n",
        "logger.info(\"Calculating parameters for Learning Rate Schedule...\")\n",
        "\n",
        "# Parameters from your config block [cite: 4, 120]\n",
        "initial_learning_rate = LEARNING_RATE\n",
        "epochs = TOTAL_EPOCHS\n",
        "batch_size = BATCH_SIZE\n",
        "# dataset_size should be available from the data pipeline setup [cite: 16]\n",
        "\n",
        "# Calculate steps\n",
        "# try:\n",
        "#     # Estimate steps per epoch. Use ceiling division for robustness.\n",
        "#     steps_per_epoch = (dataset_size + batch_size - 1) // batch_size\n",
        "#     # Determine the epoch and step where decay starts\n",
        "#     decay_start_epoch = epochs // 2\n",
        "#     decay_start_step = steps_per_epoch * decay_start_epoch\n",
        "#     # Determine the total number of steps over which decay occurs\n",
        "#     decay_epochs = epochs - decay_start_epoch\n",
        "#     total_decay_steps = steps_per_epoch * decay_epochs\n",
        "\n",
        "#     logger.info(f\"VERIFY: dataset_size = {dataset_size}\")\n",
        "#     logger.info(f\"VERIFY: BATCH_SIZE = {BATCH_SIZE}\")\n",
        "#     logger.info(f\"VERIFY: EPOCHS = {epochs}\") # Or TOTAL_EPOCHS\n",
        "#     logger.info(f\"VERIFY: steps_per_epoch = {steps_per_epoch}\")\n",
        "#     logger.info(f\"VERIFY: decay_start_step = {decay_start_step}\")\n",
        "#     logger.info(f\"VERIFY: total_decay_steps = {total_decay_steps}\")\n",
        "#     # Sanity check: decay_start_step + total_decay_steps should equal total steps in training\n",
        "#     total_steps_calculated = steps_per_epoch * epochs\n",
        "#     logger.info(f\"VERIFY: Total calculated steps = {total_steps_calculated}\")\n",
        "#     logger.info(f\"VERIFY: Decay ends at step = {decay_start_step + total_decay_steps}\")\n",
        "\n",
        "\n",
        "# except NameError as e:\n",
        "#     logger.error(f\"Missing required variable for LR schedule calculation (dataset_size?): {e}\", exc_info=True)\n",
        "#     raise\n",
        "# except Exception as e:\n",
        "#      logger.error(f\"Error calculating LR schedule steps: {e}\", exc_info=True)\n",
        "#      raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoNIll21a_6B"
      },
      "source": [
        "## Build Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9P16rXtHKTb"
      },
      "outputs": [],
      "source": [
        "# --- Test the pipeline ---\n",
        "logger.info(\"Fetching one batch element to inspect its structure...\")\n",
        "try:\n",
        "    # Fetch one element (which is one batch) WITHOUT unpacking it in the loop definition\n",
        "    for batch_element in train_dataset.take(1):\n",
        "        logger.info(f\"Successfully fetched one batch element.\")\n",
        "\n",
        "        # --- Inspect the Structure ---\n",
        "        logger.info(f\"Type of yielded batch element: {type(batch_element)}\")\n",
        "        element_len = -1 # Default value\n",
        "        if isinstance(batch_element, (tuple, list)):\n",
        "              element_len = len(batch_element)\n",
        "              logger.info(f\"Number of items in batch element tuple/list: {element_len}\")\n",
        "              # Log shapes of individual items\n",
        "              for i, item in enumerate(batch_element):\n",
        "                  if hasattr(item, 'shape'): # Check if it has a shape attribute (like Tensor)\n",
        "                      logger.info(f\"    Item {i} shape: {item.shape}\")\n",
        "                  else:\n",
        "                      logger.info(f\"    Item {i} type: {type(item)}\")\n",
        "        elif hasattr(batch_element, 'shape'):\n",
        "              # If the element itself has a shape (e.g., a single tensor was yielded)\n",
        "              logger.info(f\"Batch element shape (element is not tuple/list): {batch_element.shape}\")\n",
        "        else:\n",
        "              logger.info(f\"Batch element is not a tuple/list or tensor-like.\")\n",
        "\n",
        "        # --- Conditional Unpacking and Visualization (Only if structure seems correct) ---\n",
        "        if isinstance(batch_element, (tuple, list)) and element_len == 2:\n",
        "            logger.info(\"Structure seems correct (tuple/list of size 2). Proceeding with unpacking and visualization.\")\n",
        "            example_input, example_target = batch_element # Unpack now\n",
        "\n",
        "            logger.info(f\"  Input batch shape: {example_input.shape}\")\n",
        "            logger.info(f\"  Target batch shape: {example_target.shape}\")\n",
        "\n",
        "            # Verify the data range is approximately [-1, 1]\n",
        "            logger.info(f\"  Input min value: {tf.reduce_min(example_input).numpy():.2f}\")\n",
        "            plt.figure(figsize=(6, 6))\n",
        "            plt.subplot(1, 2, 1)\n",
        "            # De-normalize for display: (img + 1) * 127.5\n",
        "            plt.imshow((example_input[0] + 1) / 2.0)\n",
        "            plt.title(\"Sample Input\")\n",
        "            plt.axis(\"off\")\n",
        "            plt.subplot(1, 2, 2)\n",
        "            # Display target - might need grayscale if it's B&W (but loaded as 3ch here)\n",
        "            plt.imshow((example_target[0] + 1) / 2.0)\n",
        "            plt.title(\"Sample Target\")\n",
        "            plt.axis(\"off\")\n",
        "            plt.show()\n",
        "            logger.info(\"Pipeline test batch values checked and visualized successfully.\")\n",
        "        else:\n",
        "            logger.error(f\"Pipeline yielding unexpected structure! Expected tuple/list of 2 items, but got type {type(batch_element)} with length {element_len if element_len != -1 else 'N/A'}.\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error fetching or inspecting dataset element: {e}\", exc_info=True)\n",
        "    # raise # Optionally re-raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOFVw5cobkWh"
      },
      "outputs": [],
      "source": [
        "logger.info(\"Mapping functions and building the final training dataset pipeline...\")\n",
        "try:\n",
        "    # Assumes 'dataset' holds the output of tf.data.Dataset.list_files(...)\n",
        "    logger.info(f\"Listing files from: {CONCAT_IMAGE_DIR}\")\n",
        "    # Create the initial dataset object from list_files\n",
        "    # This defines the 'dataset' variable needed below\n",
        "    dataset = tf.data.Dataset.list_files(os.path.join(CONCAT_IMAGE_DIR, '*.png')) # Or appropriate pattern\n",
        "    dataset_size = tf.data.experimental.cardinality(dataset).numpy()\n",
        "    if dataset_size == 0:\n",
        "          raise ValueError(f\"No files found matching pattern in {CONCAT_IMAGE_DIR}\")\n",
        "    logger.info(f\"Found {dataset_size} image files.\")\n",
        "\n",
        "    # Set buffer size based on dataset size if needed (BUFFER_SIZE is\n",
        "    # defined before this block)\n",
        "    # BUFFER_SIZE exists from  config section\n",
        "    BUFFER_SIZE = min(BUFFER_SIZE, dataset_size)\n",
        "\n",
        "\n",
        "    logger.info(f\"Initial dataset element spec: {dataset.element_spec}\")\n",
        "\n",
        "    # --- DEBUG: Check elements AFTER list_files (should be file paths) ---\n",
        "    logger.info(\"--- Inspecting elements AFTER list_files ---\")\n",
        "    for item_path in dataset.take(2): # Look at first 2 paths\n",
        "         logger.info(f\"  Element type: {type(item_path)}, Value: {item_path.numpy().decode()}\") # Decode string tensor\n",
        "\n",
        "    # 1. Map the loading function\n",
        "    # Takes file paths -> loads raw image tensors\n",
        "    train_dataset_loaded = dataset.map(load_image, # Use 'dataset' here\n",
        "                                      num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    logger.info(f\"After load_image map, element spec: {train_dataset_loaded.element_spec}\")\n",
        "    # --- DEBUG: Check elements AFTER load_image map ---\n",
        "    logger.info(\"--- Inspecting elements AFTER load_image ---\")\n",
        "    for item_loaded in train_dataset_loaded.take(1):\n",
        "         logger.info(f\"  Element type: {type(item_loaded)}, Shape: {item_loaded.shape}\") # Expect (256, 512, 3)\n",
        "\n",
        "\n",
        "    # 2. Map the preprocessing and augmentation function\n",
        "    # Takes raw image tensors -> applies augmentation, splitting, normalization\n",
        "    # Outputs pairs of (input_image, target_image)\n",
        "    train_dataset_loaded = dataset.map(load_image,\n",
        "                                      num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    logger.info(f\"After load_image map, element spec: {train_dataset_loaded.element_spec}\")\n",
        "    # --- DEBUG: Check elements AFTER preprocess_image_train map ---\n",
        "    logger.info(\"--- Inspecting elements AFTER preprocess_image_train ---\")\n",
        "    for item_processed in train_dataset_loaded.take(1):\n",
        "         logger.info(f\"  Element type: {type(item_processed)}\") # Expect tuple\n",
        "         if isinstance(item_processed, tuple):\n",
        "             logger.info(f\"  Element length: {len(item_processed)}\") # Expect 2\n",
        "             if len(item_processed) == 2:\n",
        "                 logger.info(f\"  Item 0 shape: {item_processed[0].shape}\") # Expect (256, 256, 3)\n",
        "                 logger.info(f\"  Item 1 shape: {item_processed[1].shape}\") # Expect (256, 256, 3)\n",
        "         elif hasattr(item_processed, 'shape'):\n",
        "              logger.info(f\"  Element shape (NOT TUPLE!): {item_processed.shape}\")\n",
        "         else:\n",
        "              logger.info(f\"  Element is neither tuple nor tensor!\")\n",
        "    train_dataset_processed = train_dataset_loaded.map(preprocess_image_train,\n",
        "                                                      num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    # 3. Shuffle the dataset\n",
        "    # Uses BUFFER_SIZE defined earlier. Important for training stability.\n",
        "    train_dataset_shuffled = train_dataset_processed.shuffle(BUFFER_SIZE)\n",
        "\n",
        "    # 4. Batch the dataset\n",
        "    # Groups pairs into batches of size BATCH_SIZE\n",
        "    train_dataset_batched = train_dataset_shuffled.batch(BATCH_SIZE)\n",
        "    logger.info(f\"After batch, element spec: {train_dataset_batched.element_spec}\") # Should be tuple of batched TensorSpecs\n",
        "\n",
        "\n",
        "    # 5. Prefetch for performance\n",
        "    # Allows the CPU to prepare the next batch(es) while the GPU is busy\n",
        "    train_dataset_final = train_dataset_batched.prefetch(buffer_size=tf.data.AUTOTUNE) # Assign to final variable\n",
        "\n",
        "\n",
        "    logger.info(\"Training dataset pipeline built successfully.\")\n",
        "\n",
        "    # --- Test the pipeline ---\n",
        "    logger.info(\"Fetching one batch to test pipeline...\")\n",
        "    for example_input, example_target in train_dataset_final.take(1):\n",
        "        logger.info(f\"  Input batch shape: {example_input.shape}\") # Should be (BATCH_SIZE, 256, 256, 3)\n",
        "        logger.info(f\"  Target batch shape: {example_target.shape}\") # Should be (BATCH_SIZE, 256, 256, 3)\n",
        "        # Display one example from the batch\n",
        "        plt.figure(figsize=(6, 6))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        # De-normalize for display: (img + 1) * 127.5\n",
        "        plt.imshow((example_input[0] + 1) / 2.0)\n",
        "        plt.title(\"Sample Input\")\n",
        "        plt.axis(\"off\")\n",
        "        plt.subplot(1, 2, 2)\n",
        "        # Display target - might need grayscale if it's B&W (but loaded as 3ch here)\n",
        "        plt.imshow((example_target[0] + 1) / 2.0, cmap='gray' if example_target.shape[-1]==1 else None)\n",
        "        plt.title(\"Sample Target\")\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()\n",
        "    logger.info(\"Pipeline test batch fetched successfully.\")\n",
        "    # After fetching the batch:\n",
        "    logger.info(f\"  Target Batch Min Value (Normalized): {tf.reduce_min(example_target).numpy():.4f}\")\n",
        "    logger.info(f\"  Target Batch Max Value (Normalized): {tf.reduce_max(example_target).numpy():.4f}\")\n",
        "    # Also visualize the DENORMALIZED target again to be sure it looks right\n",
        "    plt.imshow((example_target[0].numpy() + 1) / 2.0)\n",
        "    plt.title(\"VERIFY Sample Target Looks Correct\")\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error building or testing dataset pipeline: {e}\", exc_info=True)\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2cFbNBgFzuT"
      },
      "source": [
        "## Define generator and discriminator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnAO5YDfHS2n"
      },
      "source": [
        "### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPcbfAKbHUqA"
      },
      "outputs": [],
      "source": [
        "# Define the downsampling block (Encoder)\n",
        "def downsample(filters, size, apply_norm=True, apply_dropout=False, name=None):\n",
        "    '''\n",
        "    Implements a Conv2D layer with strides=2 for downsampling, followed by\n",
        "    optional normalization (BatchNormalization in this baseline) and LeakyReLU\n",
        "    activation. Bias is typically turned off in conv layers when followed by\n",
        "    normalization.\n",
        "    '''\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    result = tf.keras.Sequential(name=name)\n",
        "    result.add(layers.Conv2D(filters, size, strides=2, padding='same',\n",
        "                             kernel_initializer=initializer, use_bias=not apply_norm)) # No bias if using norm\n",
        "    if apply_norm:\n",
        "        if use_batchnorm:\n",
        "             # Using BatchNormalization. Consider momentum and epsilon defaults or tuning later.\n",
        "             result.add(layers.BatchNormalization())\n",
        "        # else: Add InstanceNormalization/LayerNormalization here if switching later\n",
        "        else:\n",
        "            result.add(layers.GroupNormalization(groups=-1))\n",
        "    if apply_dropout:\n",
        "      result.add(layers.Dropout(0.5)) # Standard dropout rate for pix2pix\n",
        "    result.add(layers.LeakyReLU()) # LeakyReLU in encoder\n",
        "    return result\n",
        "\n",
        "# Define the upsampling block (Decoder)\n",
        "def upsample(filters, size, apply_dropout=False, name=None):\n",
        "  '''\n",
        "  Implements Conv2DTranspose with strides=2 for upsampling, followed by\n",
        "  optional normalization and optional Dropout (used in the first few decoder\n",
        "  layers as per the pix2pix paper), and finally ReLU activation (as used in\n",
        "  the pix2pix decoder).\n",
        "  '''\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "  result = tf.keras.Sequential(name=name)\n",
        "  # 1. Upsample first using bilinear interpolation (doubles H, W)\n",
        "  result.add(layers.UpSampling2D(size=2, interpolation='bilinear'))\n",
        "  # 2. Apply standard Conv2D with strides=1 (preserves H, W)\n",
        "  result.add(layers.Conv2D(filters, size, strides=1, # <<< STRIDES MUST BE 1 HERE\n",
        "                            padding='same',\n",
        "                            kernel_initializer=initializer,\n",
        "                            # Use bias=False if Norm layer includes scale/center\n",
        "                            use_bias=False))\n",
        "\n",
        "  if use_batchnorm:\n",
        "      result.add(layers.BatchNormalization())\n",
        "  else:\n",
        "      result.add(layers.GroupNormalization(groups=-1))\n",
        "  # else: Add InstanceNorm/LayerNorm here if switching later\n",
        "\n",
        "  if apply_dropout:\n",
        "      result.add(layers.Dropout(0.5)) # Standard dropout rate for pix2pix decoder\n",
        "  result.add(layers.ReLU()) # ReLU in decoder, as per original paper\n",
        "  return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcqQtuu1HjyG"
      },
      "source": [
        "### Build Generator (U-net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bxUw-j3Hswh"
      },
      "outputs": [],
      "source": [
        "logger.info(\"Defining Generator (U-Net) model...\")\n",
        "\n",
        "# Build the U-Net Generator Model using the Keras Functional API\n",
        "def build_generator(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), output_channels=OUTPUT_CHANNELS, name=\"unet_generator\"):\n",
        "  '''\n",
        "  Uses the Keras Functional API (Input, Model) which is well-suited for models\n",
        "  with non-linear topology like skip connections.\n",
        "\n",
        "  Defines an Input layer matching our image shape (256, 256, 3).\n",
        "\n",
        "  Creates a down_stack by calling the downsample function repeatedly,\n",
        "  progressively reducing spatial dimensions and increasing filter counts. Each\n",
        "  layer's output is stored in the skips list.\n",
        "\n",
        "  Creates an up_stack using the upsample function.\n",
        "\n",
        "  The core U-Net logic iterates through the up_stack, takes the output x,\n",
        "  concatenates it with the corresponding feature map from the skips list\n",
        "  (layers.Concatenate()), and passes the result to the next upsample layer.\n",
        "\n",
        "  A final Conv2DTranspose layer (last) upsamples to the full image size\n",
        "  (256x256) with the desired number of output_channels (3) and uses a tanh\n",
        "  activation function to ensure the output pixel values are in the range\n",
        "  [-1, 1], matching our data normalization.\n",
        "  '''\n",
        "  with tf.device('/gpu:0'):\n",
        "    gf = GENERATOR_FILTERS_INITIAL\n",
        "    logger.info(f\"Building Generator with input shape {input_shape} and output channels {output_channels}\")\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # Encoder path (downsampling) - filter counts double each time\n",
        "    down_stack = [\n",
        "        downsample(gf    , 4, apply_norm=False, name=\"down_1\"), # (bs, 128, 128, gf)\n",
        "        downsample(gf * 2, 4, name=\"down_2\"),                   # (bs, 64, 64, gf*2)\n",
        "        downsample(gf * 4, 4, name=\"down_3\"),                   # (bs, 32, 32, gf*4)\n",
        "        downsample(min(gf * 8, 512), 4, name=\"down_4\"),         # (bs, 16, 16, gf*8 or 512)\n",
        "        downsample(min(gf * 8, 512), 4, name=\"down_5\"),         # (bs, 8, 8, gf*8 or 512)\n",
        "        downsample(min(gf * 8, 512), 4, name=\"down_6\"),         # (bs, 4, 4, gf*8 or 512)\n",
        "        downsample(min(gf * 8, 512), 4, name=\"down_7\"),         # (bs, 2, 2, gf*8 or 512)\n",
        "        downsample(min(gf * 8, 512), 4, apply_dropout=True, name=\"down_8\"),         # (bs, 1, 1, gf*8 or 512) - Bottleneck\n",
        "    ]\n",
        "\n",
        "    # Decoder path (upsampling) - filter counts halve each time\n",
        "    up_stack = [\n",
        "        upsample(min(gf * 8, 512), 4, apply_dropout=True, name=\"up_1\"), # (bs, 2, 2, gf*8 or 512)\n",
        "        upsample(min(gf * 8, 512), 4, apply_dropout=True, name=\"up_2\"), # (bs, 4, 4, gf*8 or 512)\n",
        "        upsample(min(gf * 8, 512), 4, apply_dropout=True, name=\"up_3\"), # (bs, 8, 8, gf*8 or 512)\n",
        "        upsample(min(gf * 8, 512), 4, name=\"up_4\"),                     # (bs, 16, 16, gf*8 or 512)\n",
        "        upsample(gf * 4, 4, name=\"up_5\"),                               # (bs, 32, 32, gf*4)\n",
        "        upsample(gf * 2, 4, name=\"up_6\"),                               # (bs, 64, 64, gf*2)\n",
        "        upsample(gf    , 4, name=\"up_7\"),                               # (bs, 128, 128, gf)\n",
        "\n",
        "    ]\n",
        "\n",
        "    # Final Output Layer\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    last = layers.Conv2DTranspose(output_channels, 4,\n",
        "                                    strides=2,\n",
        "                                    padding='same',\n",
        "                                    kernel_initializer=initializer,\n",
        "                                    activation='tanh', # Output activation is tanh for [-1, 1] range\n",
        "                                    name=\"final_output_convt\") # (bs, 256, 256, output_channels)\n",
        "\n",
        "    x = inputs\n",
        "    skips = []\n",
        "    # Downsampling through the model, collecting skip connections\n",
        "    for i, down in enumerate(down_stack):\n",
        "        x = down(x)\n",
        "        logger.debug(f\"Generator Encoder Layer {i+1} output shape: {x.shape}\")\n",
        "        skips.append(x)\n",
        "\n",
        "    # Convert reversed iterator to an explicit list BEFORE zipping\n",
        "    skips_for_concat = list(reversed(skips[:-1])) # Use list() here\n",
        "    logger.info(f\"Num skips: {len(skips_for_concat)}, Num up-blocks: {len(up_stack)}\")\n",
        "    if len(up_stack) != len(skips_for_concat):\n",
        "      raise ValueError(\"Skip/Up count mismatch!\")\n",
        "\n",
        "\n",
        "    # Upsampling and establishing the skip connections\n",
        "    for i, (up, skip) in enumerate(zip(up_stack, skips_for_concat)):\n",
        "        x = up(x) # Apply upsampling block (using UpSampling2D+Conv2D)\n",
        "        logger.debug(f\"Decoder Step {i+1}: Upsampled shape: {x.shape}, Skip shape: {skip.shape}\")\n",
        "        # shape check\n",
        "        if x.shape[1:3] != skip.shape[1:3]:\n",
        "            error_msg = (f\"Shape mismatch before Concat {i+1}! Up: {x.shape}, Skip: {skip.shape}\")\n",
        "            logger.error(error_msg)\n",
        "            raise ValueError(error_msg)\n",
        "        # Concatenate the skip connection (from corresponding encoder layer)\n",
        "        x = layers.Concatenate()([x, skip]) # Concatenate\n",
        "        logger.debug(f\"Generator Decoder Layer {i+1} output shape after concat: {x.shape}\")\n",
        "\n",
        "\n",
        "    x = last(x) # Apply the final output layer\n",
        "    logger.debug(f\"Generator Final Output shape: {x.shape}\")\n",
        "\n",
        "    return Model(inputs=inputs, outputs=x, name=name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hJfD5RMIczz"
      },
      "source": [
        "### Build Discriminator (PatchGAN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTWBQvGGF3pP"
      },
      "outputs": [],
      "source": [
        "\n",
        "logger.info(\"Defining Discriminator (PatchGAN) model...\")\n",
        "# --- Discriminator (PatchGAN) ---\n",
        "\n",
        "# Build the PatchGAN Discriminator Model using the Keras Functional API\n",
        "def build_discriminator(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), target_shape=(IMG_HEIGHT, IMG_WIDTH, 3), name=\"patchgan_discriminator\"):\n",
        "    '''\n",
        "    Uses the Functional API. It takes two inputs: inp (the source photo) and tar\n",
        "    (either the real target diagram or the fake one from the generator).\n",
        "\n",
        "    These two inputs are concatenated along the channel axis using\n",
        "    layers.Concatenate(), resulting in a tensor with 6 channels ((256, 256, 6)).\n",
        "\n",
        "    Applies a few downsample blocks to reduce spatial dimensions and extract\n",
        "    features.\n",
        "\n",
        "    The PatchGAN structure commonly involves some Conv2D layers with strides=1\n",
        "    and specific padding (ZeroPadding2D) towards the end. This allows the\n",
        "    network's receptive field to grow and cover larger patches of the input\n",
        "    without further drastic downsampling.\n",
        "\n",
        "    The final layer is a Conv2D with a single output channel (1) and no\n",
        "    activation function. This produces a grid of logits (raw scores), where each\n",
        "    value represents the discriminator's \"realness\" prediction for a\n",
        "    corresponding patch in the input images. The output shape (e.g., (30, 30, 1))\n",
        "    depends on the exact layers/strides/padding used.\n",
        "    '''\n",
        "    logger.info(f\"Building Discriminator with input shape {input_shape} and target shape {target_shape}\")\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "    inp = Input(shape=input_shape, name='input_image')\n",
        "    tar = Input(shape=target_shape, name='target_image')\n",
        "\n",
        "    x = layers.Concatenate()([inp, tar])\n",
        "    logger.debug(f\"Discriminator input shape after concat: {x.shape}\")\n",
        "\n",
        "    # Layer 1 (Conv2D -> LeakyReLU) - Apply SN\n",
        "    # Wrap the Conv2D layer directly with SpectralNormalization\n",
        "    x = SpectralNormalization(layers.Conv2D(64, 4, strides=2, padding='same',\n",
        "                                            kernel_initializer=initializer, use_bias=True, # Bias ok w/o BN\n",
        "                                            name=\"disc_conv_1\"))(x)\n",
        "    x = layers.LeakyReLU(name=\"disc_leaky_1\")(x)\n",
        "    logger.debug(f\"Discriminator Layer 1 SN output shape: {x.shape}\")\n",
        "\n",
        "    # Layer 2 (Conv2D -> Norm -> LeakyReLU) - Apply SN\n",
        "    x = SpectralNormalization(layers.Conv2D(128, 4, strides=2, padding='same',\n",
        "                                            kernel_initializer=initializer, use_bias=not use_batchnorm,\n",
        "                                            name=\"disc_conv_2\"))(x)\n",
        "    if use_batchnorm: x = layers.BatchNormalization(name=\"disc_norm_2\")(x)\n",
        "    x = layers.LeakyReLU(name=\"disc_leaky_2\")(x)\n",
        "    logger.debug(f\"Discriminator Layer 2 SN output shape: {x.shape}\")\n",
        "\n",
        "    # Layer 3 (Conv2D -> Norm -> LeakyReLU) - Apply SN\n",
        "    x = SpectralNormalization(layers.Conv2D(256, 4, strides=1, padding='same',\n",
        "                                            kernel_initializer=initializer, use_bias=not use_batchnorm,\n",
        "                                            name=\"disc_conv_3\"))(x)\n",
        "    if use_batchnorm: x = layers.BatchNormalization(name=\"disc_norm_3\")(x)\n",
        "    x = layers.LeakyReLU(name=\"disc_leaky_3\")(x)\n",
        "    logger.debug(f\"Discriminator Layer 3 SN output shape: {x.shape}\")\n",
        "\n",
        "    # Layer 4 (Padding -> Conv2D -> Norm -> LeakyReLU) - Apply SN\n",
        "    #zero_pad1 = layers.ZeroPadding2D(name=\"disc_pad_1\")(x)\n",
        "    conv = SpectralNormalization(layers.Conv2D(512, 4, strides=1,\n",
        "                                            kernel_initializer=initializer,\n",
        "                                            use_bias=not use_batchnorm, name=\"disc_conv_4\"))(x)\n",
        "    logger.debug(f\"Discriminator Conv4 SN output shape: {conv.shape}\")\n",
        "\n",
        "    if use_batchnorm: norm1 = layers.BatchNormalization(name=\"disc_norm_4\")(conv)\n",
        "    else: norm1 = conv\n",
        "    leaky_relu = layers.LeakyReLU(name=\"disc_leaky_4\")(norm1)\n",
        "\n",
        "    # Final Output Layer (Padding -> Conv2D) - Apply SN\n",
        "    zero_pad2 = layers.ZeroPadding2D(name=\"disc_pad_2\")(leaky_relu)\n",
        "    last = SpectralNormalization(layers.Conv2D(1, 4, strides=1, padding = 'valid',\n",
        "                                            kernel_initializer=initializer, name=\"disc_output_conv\"))(leaky_relu)\n",
        "    logger.debug(f\"Discriminator Final SN Output shape: {last.shape}\")\n",
        "\n",
        "    return Model(inputs=[inp, tar], outputs=last, name=name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BAQkTfuhevt"
      },
      "outputs": [],
      "source": [
        "def build_discriminator_70x70(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3),\n",
        "                              target_shape=(IMG_HEIGHT, IMG_WIDTH, 3),\n",
        "                              name=\"patchgan_discriminator_70x70\"):\n",
        "    '''\n",
        "    Builds a PatchGAN discriminator with a 70x70 receptive field.\n",
        "\n",
        "    Uses the Functional API. It takes two inputs: inp (the source photo) and tar\n",
        "    (either the real target diagram or the fake one from the generator).\n",
        "\n",
        "    These two inputs are concatenated along the channel axis.\n",
        "\n",
        "    Applies downsampling blocks. The key change for a 70x70 RF (compared to the\n",
        "    previous 46x46) is using stride=2 in the third convolutional layer.\n",
        "\n",
        "    The final layer is a Conv2D with a single output channel (1) and no\n",
        "    activation function, producing a grid of logits. For a 256x256 input, this\n",
        "    architecture typically yields a 30x30 output grid (specifically (256/8)-3 = 29x29).\n",
        "    '''\n",
        "    logger.info(f\"Building Discriminator with input shape {input_shape}, target shape {target_shape}, aiming for 70x70 RF\")\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "    inp = Input(shape=input_shape, name='input_image')\n",
        "    tar = Input(shape=target_shape, name='target_image')\n",
        "\n",
        "    # Concatenate source image and target/generated image\n",
        "    x = layers.Concatenate()([inp, tar]) # Shape: (H, W, 6)\n",
        "    logger.debug(f\"Discriminator input shape after concat: {x.shape}\")\n",
        "\n",
        "    # Layer 1 (C64): Conv2D -> LeakyReLU - Apply SN\n",
        "    # Output size: H/2 x W/2\n",
        "    # RF = 4, Jump = 2\n",
        "    x = SpectralNormalization(layers.Conv2D(64, 4, strides=2, padding='same',\n",
        "                                            kernel_initializer=initializer, use_bias=True, # Bias ok w/o BN\n",
        "                                            name=\"disc_conv_1\"))(x)\n",
        "    x = layers.LeakyReLU(name=\"disc_leaky_1\")(x)\n",
        "    logger.debug(f\"Discriminator Layer 1 SN output shape: {x.shape}\") # e.g., (128, 128, 64)\n",
        "\n",
        "    # Layer 2 (C128): Conv2D -> Norm -> LeakyReLU - Apply SN\n",
        "    # Output size: H/4 x W/4\n",
        "    # RF = 10, Jump = 4\n",
        "    x = SpectralNormalization(layers.Conv2D(128, 4, strides=2, padding='same',\n",
        "                                            kernel_initializer=initializer, use_bias=not use_batchnorm,\n",
        "                                            name=\"disc_conv_2\"))(x)\n",
        "    if use_batchnorm: x = layers.BatchNormalization(name=\"disc_norm_2\")(x)\n",
        "    x = layers.LeakyReLU(name=\"disc_leaky_2\")(x)\n",
        "    logger.debug(f\"Discriminator Layer 2 SN output shape: {x.shape}\") # e.g., (64, 64, 128)\n",
        "\n",
        "    # Layer 3 (C256): Conv2D -> Norm -> LeakyReLU - Apply SN\n",
        "    # *** MODIFIED: Changed strides from 1 to 2 for 70x70 RF ***\n",
        "    # Output size: H/8 x W/8\n",
        "    # RF = 22, Jump = 8\n",
        "    x = SpectralNormalization(layers.Conv2D(256, 4, strides=2, padding='same', # <-- Stride changed here\n",
        "                                            kernel_initializer=initializer, use_bias=not use_batchnorm,\n",
        "                                            name=\"disc_conv_3\"))(x)\n",
        "    if use_batchnorm: x = layers.BatchNormalization(name=\"disc_norm_3\")(x)\n",
        "    x = layers.LeakyReLU(name=\"disc_leaky_3\")(x)\n",
        "    logger.debug(f\"Discriminator Layer 3 SN output shape: {x.shape}\") # e.g., (32, 32, 256)\n",
        "\n",
        "    # Layer 4 (C512): Conv2D -> Norm -> LeakyReLU - Apply SN\n",
        "    # Output size: H/8 x W/8 (stride 1)\n",
        "    # RF = 46, Jump = 8\n",
        "    # Note: Original code had commented-out ZeroPadding here. Using padding='same' in Conv2D.\n",
        "    conv = SpectralNormalization(layers.Conv2D(512, 4, strides=1, padding='same', # <-- padding='same' here\n",
        "                                            kernel_initializer=initializer,\n",
        "                                            use_bias=not use_batchnorm, name=\"disc_conv_4\"))(x)\n",
        "    logger.debug(f\"Discriminator Conv4 SN output shape: {conv.shape}\") # e.g., (32, 32, 512)\n",
        "\n",
        "    if use_batchnorm: norm1 = layers.BatchNormalization(name=\"disc_norm_4\")(conv)\n",
        "    else: norm1 = conv # Assign conv to norm1 if not using batchnorm\n",
        "    leaky_relu = layers.LeakyReLU(name=\"disc_leaky_4\")(norm1)\n",
        "\n",
        "    # Final Output Layer: Conv2D - Apply SN\n",
        "    # Output size: (H/8 - 3) x (W/8 - 3) due to k=4, s=1, padding='valid'\n",
        "    # RF = 70, Jump = 8\n",
        "    # Note: Original code had ZeroPadding2D defined but not used for input here. Using leaky_relu directly.\n",
        "    last = SpectralNormalization(layers.Conv2D(1, 4, strides=1, padding='valid', # <-- padding='valid' here\n",
        "                                            kernel_initializer=initializer, name=\"disc_output_conv\"))(leaky_relu)\n",
        "    logger.debug(f\"Discriminator Final SN Output shape: {last.shape}\") # e.g., (29, 29, 1) for 256x256 input\n",
        "\n",
        "    return Model(inputs=[inp, tar], outputs=last, name=name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilhUE7GWJbwX"
      },
      "source": [
        "## Loss Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PsDTkQGLCgn"
      },
      "source": [
        "#### Explanation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXbgKWrfJhs4"
      },
      "source": [
        "Let's define the loss functions for our baseline pix2pix model. We need an adversarial loss to make the diagrams look realistic (like real diagrams) and a reconstruction loss to ensure the generated diagram matches the content of the input photo.\n",
        "\n",
        "Considering our goal (generating 2D structural diagrams) and the need for a stable, standard baseline:\n",
        "\n",
        "1. **Adversarial Loss: LSGAN (Least Squares GAN)**\n",
        "\n",
        "- **Why LSGAN for baseline?** Compared to Binary Cross-Entropy (BCE), LSGAN is often found to be more stable during training and less prone to vanishing gradients, especially early on. It provides smoother gradients and can lead to higher quality results. For a baseline, stability is valuable.\n",
        "\n",
        "- **How it works:** Instead of classifying patches as 0 (fake) or 1 (real) with a sigmoid cross-entropy, it uses a Mean Squared Error (MSE) loss. The discriminator tries to make its output close to 1 for real pairs and 0 for fake pairs (generated). The generator tries to make the discriminator output 1 for its fake pairs.\n",
        "\n",
        "- **Implementation:** We'll use `tf.keras.losses.MeanSquaredError`.\n",
        "\n",
        "  - **Discriminator Loss** (`L_D`):\n",
        "    - Wants `D(real_photo, real_diagram)` output to be close to 1.\n",
        "    - Wants `D(real_photo, generated_diagram)` output to be close to 0.\n",
        "    - `loss_D_real = mse(D(real_photo, real_diagram), tf.ones_like(D_output))`\n",
        "    - `loss_D_fake = mse(D(real_photo, generated_diagram), tf.zeros_like(D_output))`  \n",
        "    - `L_D = 0.5 * (loss_D_real + loss_D_fake)` (The 0.5 scaling is common but optional)\n",
        "  - **Generator Adversarial Loss** (`L_G_adversarial`):\n",
        "    - Wants `D(real_photo, generated_diagram)` output to be close to 1 (to fool the discriminator).\n",
        "    - `L_G_adversarial = mse(D(real_photo, generated_diagram), tf.ones_like(D_output))`\n",
        "2. **Reconstruction Loss: L1 Loss (Mean Absolute Error)**\n",
        "\n",
        "- **Why L1 for baseline?** While we note concerns about potential blurriness (which we can address later by adding Edge loss or tuning), L1 is the standard reconstruction loss used in the original pix2pix paper.\n",
        "  - It provides strong structural guidance, forcing the generator's output pixels to be close to the target diagram's pixels.\n",
        "  - It generally produces less blurry results than L2 (MSE) loss.\n",
        "  - It establishes a standard baseline performance metric before introducing more specialized losses.\n",
        "- **How it works:** Calculates the average absolute difference between each pixel in the generated diagram and the real target diagram. `Mean(|real_diagram - generated_diagram|)`.\n",
        "- **Implementation:** We'll use `tf.keras.losses.MeanAbsoluteError` or `tf.reduce_mean(tf.abs(...))`.\n",
        "- **Weight** (`LAMBDA_L1`): This hyperparameter balances the adversarial loss and the L1 loss. The original paper found `LAMBDA_L1 = 100` worked well, heavily emphasizing reconstruction accuracy. We will start with this for the baseline.\n",
        "    - `L_L1 = LAMBDA_L1 * mae(real_diagram, generated_diagram)`\n",
        "3. Total Generator Loss:\n",
        "- The generator aims to minimize both its adversarial loss and the reconstruction loss.\n",
        "- `L_G_Total = L_G_adversarial + L_L1`\n",
        "- `L_G_Total = mse(D(fake), 1.0) + LAMBDA_L1 * mae(target, generated)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFG5f9vYLIqH"
      },
      "source": [
        "#### Code for loss functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qac8Z6MQJuJH"
      },
      "outputs": [],
      "source": [
        "# === EDGE LOSS FUNCTION ===\n",
        "logger.info(\"Defining Edge Loss function...\")\n",
        "\n",
        "@tf.function\n",
        "def calculate_edge_loss(target, generated):\n",
        "    \"\"\"\n",
        "    Calculates the L1 loss between the Sobel edges of the target and generated images.\n",
        "\n",
        "    Args:\n",
        "        target: The ground truth target image tensor (shape [batch, H, W, C], range [-1, 1]).\n",
        "        generated: The image tensor output by the generator (shape [batch, H, W, C], range [-1, 1]).\n",
        "\n",
        "    Returns:\n",
        "        A scalar tensor representing the mean absolute error between gradients.\n",
        "    \"\"\"\n",
        "    # 1. Convert images to grayscale first (Sobel works best on single channel)\n",
        "    #    Assuming input images are RGB (3 channels) in range [-1, 1]\n",
        "    #    Convert to [0, 1] range first for rgb_to_grayscale\n",
        "    target_gray = tf.image.rgb_to_grayscale((target + 1.0) / 2.0)\n",
        "    generated_gray = tf.image.rgb_to_grayscale((generated + 1.0) / 2.0)\n",
        "    logger.debug(f\"Edge Loss - Grayscale shapes: Target {target_gray.shape}, Generated {generated_gray.shape}\")\n",
        "\n",
        "    # 2. Calculate Sobel edges (gradients in Y and X directions)\n",
        "    #    Output shape: [batch, H, W, 1, 2] (1 channel, 2 gradient dims)\n",
        "    target_sobel = tf.image.sobel_edges(target_gray)\n",
        "    generated_sobel = tf.image.sobel_edges(generated_gray)\n",
        "    logger.debug(f\"Edge Loss - Sobel shapes: Target {target_sobel.shape}, Generated {generated_sobel.shape}\")\n",
        "    target_sobel_f32 = tf.cast(target_sobel, tf.float32)\n",
        "    generated_sobel_f32 = tf.cast(generated_sobel, tf.float32)\n",
        "\n",
        "    # 3. Calculate the L1 difference between the Sobel gradients\n",
        "    #    Use tf.abs for absolute difference, then tf.reduce_mean for average error\n",
        "    edge_loss = tf.reduce_mean(tf.abs(target_sobel_f32 - generated_sobel_f32))\n",
        "    #logger.debug(f\"Edge Loss - Calculated scalar value: {edge_loss:.4f}\") # Note: Prints symbolic tensor info during trace\n",
        "\n",
        "    return edge_loss\n",
        "\n",
        "logger.info(\"Edge Loss function defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hrrCNO1w7BM"
      },
      "outputs": [],
      "source": [
        "@tf.function # Decorate for potential performance improvement\n",
        "def multi_layer_perceptual_loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculates multi-layer VGG-based perceptual loss.\n",
        "\n",
        "    Args:\n",
        "        y_true: Ground truth image tensor (shape [batch, H, W, C], range [-1, 1]).\n",
        "        y_pred: Predicted image tensor (shape [batch, H, W, C], range [-1, 1]).\n",
        "\n",
        "    Returns:\n",
        "        A scalar tensor representing the weighted sum of MSE losses across selected VGG layers.\n",
        "    \"\"\"\n",
        "    # 1. Rescale images from [-1, 1] to [0, 255]\n",
        "    y_true_0_255 = (y_true + 1.0) * 127.5\n",
        "    y_pred_0_255 = (y_pred + 1.0) * 127.5\n",
        "\n",
        "    # 2. Handle Grayscale Input (Repeat channel if necessary)\n",
        "    if y_true_0_255.shape[-1] == 1:\n",
        "        y_true_rgb = tf.repeat(y_true_0_255, 3, axis=-1)\n",
        "        y_pred_rgb = tf.repeat(y_pred_0_255, 3, axis=-1)\n",
        "    else:\n",
        "        y_true_rgb = y_true_0_255\n",
        "        y_pred_rgb = y_pred_0_255\n",
        "\n",
        "    # 3. Preprocess images for VGG16\n",
        "    y_true_processed = preprocess_input(y_true_rgb)\n",
        "    y_pred_processed = preprocess_input(y_pred_rgb)\n",
        "\n",
        "    # 4. Extract features using the pre-built multi-output VGG model\n",
        "    #    This returns a list of feature tensors (one for each layer in VGG_LAYERS_FOR_LOSS)\n",
        "    true_features_list = vgg_loss_model(y_true_processed)\n",
        "    pred_features_list = vgg_loss_model(y_pred_processed)\n",
        "\n",
        "    # Ensure the output is a list (Keras sometimes returns single tensor if list has 1 item)\n",
        "    if not isinstance(true_features_list, list):\n",
        "        true_features_list = [true_features_list]\n",
        "        pred_features_list = [pred_features_list]\n",
        "\n",
        "    # 5. Calculate weighted MSE loss for each layer's features\n",
        "    total_loss = tf.constant(0.0, dtype=true_features_list[0].dtype)\n",
        "    for i in range(len(LOSS_LAYERS)):\n",
        "        layer_loss = tf.math.reduce_mean(tf.math.square(true_features_list[i] - pred_features_list[i]))\n",
        "        total_loss += LAYER_WEIGHTS[i] * layer_loss\n",
        "\n",
        "        # Optional: Log individual layer losses if needed for debugging\n",
        "        # tf.print(f\"Layer {VGG_LAYERS_FOR_LOSS[i]} Loss: \", layer_loss)\n",
        "\n",
        "    return total_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1_SVga0LGMT"
      },
      "outputs": [],
      "source": [
        "# === LOSS FUNCTIONS ===\n",
        "logger.info(\"Defining loss functions...\")\n",
        "\n",
        "# Use MeanSquaredError for LSGAN adversarial loss\n",
        "bce = tf.keras.losses.BinaryCrossentropy(from_logits=True) # Keep for potential comparison later\n",
        "mse = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "# Use MeanAbsoluteError for L1 reconstruction loss\n",
        "mae = tf.keras.losses.MeanAbsoluteError()\n",
        "\n",
        "def discriminator_loss(disc_real_output, disc_generated_output):\n",
        "  \"\"\"Calculates LSGAN discriminator loss.\"\"\"\n",
        "  # Discriminator wants real outputs to be close to 1\n",
        "  real_loss = bce(tf.ones_like(disc_real_output), disc_real_output)\n",
        "  # Discriminator wants generated outputs to be close to 0\n",
        "  generated_loss = bce(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
        "  # Combine losses\n",
        "  total_disc_loss = real_loss + generated_loss\n",
        "  # Scale by 0.5 as commonly done (optional, affects magnitude but not gradients direction)\n",
        "  # return total_disc_loss * 0.5\n",
        "  #logger.debug(f\"Discriminator Loss - Real: {real_loss:.4f}, Fake: {generated_loss:.4f}, Total: {total_disc_loss:.4f}\")\n",
        "  return total_disc_loss\n",
        "\n",
        "def generator_loss(disc_generated_output, gen_output, target):\n",
        "  \"\"\"Calculates LSGAN Generator adversarial loss + L1 reconstruction loss.\"\"\"\n",
        "  # Generator wants discriminator to think generated images are real (output close to 1)\n",
        "  gan_loss = mse(tf.ones_like(disc_generated_output), disc_generated_output)\n",
        "  gan_loss = tf.cast(gan_loss,  dtype=bf16)\n",
        "  # L1 loss (Mean Absolute Error) between generated and target images\n",
        "  l1_loss = mae(target, gen_output)\n",
        "  l1_loss = tf.cast(l1_loss,  dtype=bf16)\n",
        "  # Calculate Perceptual Loss\n",
        "  gen_perceptual_loss = multi_layer_perceptual_loss(target, gen_output)\n",
        "  # --- Calculate Edge loss ---\n",
        "  # >> Call calculate_edge_loss and assign the result to a variable <<\n",
        "  #    (Make sure the function 'calculate_edge_loss' is defined earlier)\n",
        "  edge_loss_value = calculate_edge_loss(target, gen_output)\n",
        "  edge_loss_value = tf.cast(edge_loss_value, dtype=bf16)\n",
        "  lambda_l1 = tf.cast(LAMBDA_L1,  dtype=bf16)\n",
        "  lambda_edge = tf.cast(LAMBDA_EDGE,  dtype=bf16)\n",
        "  lambda_p = tf.cast(LAMBDA_P,  dtype=bf16)\n",
        "  # --- Combine losses ---\n",
        "  # Ensure LAMBDA_L1 and LAMBDA_EDGE are accessible here (e.g., global or passed as args)\n",
        "  total_gen_loss = gan_loss + (lambda_l1 * l1_loss) + (lambda_edge * edge_loss_value) + (lambda_p * gen_perceptual_loss)\n",
        "\n",
        "  #logger.debug(f\"Generator Loss - Adversarial: {gan_loss:.4f}, L1: {l1_loss:.4f} (W: {LAMBDA_L1*l1_loss:.4f}), Edge: {edge_loss:.4f} (W: {LAMBDA_EDGE*edge_loss:.4f}), Total: {total_gen_loss:.4f}\")\n",
        "  return total_gen_loss, gan_loss, l1_loss, edge_loss_value, gen_perceptual_loss # Return components for logging\n",
        "\n",
        "logger.info(f\"Loss functions defined: LSGAN (MSE-based) + L1 (MAE-based) with LAMBDA_L1 = {LAMBDA_L1} and Edge Loss with LAMBDA_EDGE = {LAMBDA_EDGE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hh0-Dw0YdJZg"
      },
      "source": [
        "# Custom Learning Rate Schedule Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHMnfcPLdLpz"
      },
      "outputs": [],
      "source": [
        "# Redefine the class with tf.print for debugging\n",
        "# class Pix2PixSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "#     \"\"\"\n",
        "#     Custom LR Schedule: Constant initial rate then linear decay to zero.\n",
        "#     Includes tf.print statements for debugging LR calculation.\n",
        "#     \"\"\"\n",
        "#     def __init__(self, initial_learning_rate, decay_start_step, decay_steps, name=\"Pix2PixSchedule\"):\n",
        "#         super().__init__()\n",
        "#         self.initial_learning_rate = tf.cast(initial_learning_rate, tf.float32)\n",
        "#         self.decay_start_step = tf.cast(decay_start_step, tf.float32)\n",
        "#         self.decay_steps = tf.cast(decay_steps, tf.float32)\n",
        "#         self.name = name\n",
        "#         # Log parameters during init\n",
        "#         tf.print(\"Pix2PixSchedule Initialized:\")\n",
        "#         tf.print(\"  Initial LR:\", self.initial_learning_rate)\n",
        "#         tf.print(\"  Decay Start Step:\", self.decay_start_step)\n",
        "#         tf.print(\"  Decay Steps:\", self.decay_steps)\n",
        "\n",
        "\n",
        "#     def __call__(self, step):\n",
        "#         \"\"\"Calculates the learning rate for a given step.\"\"\"\n",
        "#         with tf.name_scope(self.name or \"Pix2PixSchedule\"):\n",
        "#             step_float = tf.cast(step, tf.float32)\n",
        "#             # --- Debug Prints ---\n",
        "#             tf.print(\"Pix2PixSchedule Call - Input Step:\", step_float, output_stream=sys.stdout)\n",
        "\n",
        "#             cond = tf.less(step_float, self.decay_start_step)\n",
        "#             # tf.print(\"Pix2PixSchedule Call - Decay Condition (<\", self.decay_start_step, \"?):\", cond, output_stream=sys.stdout)\n",
        "\n",
        "#             lr_if_true = self.initial_learning_rate\n",
        "\n",
        "#             progress_in_decay = (step_float - self.decay_start_step) / self.decay_steps\n",
        "#             decay_factor = 1.0 - progress_in_decay\n",
        "#             decay_factor = tf.maximum(0.0, decay_factor) # Clamp at 0\n",
        "#             lr_if_false = self.initial_learning_rate * decay_factor\n",
        "#             # --- Debug Prints ---\n",
        "#             tf.print(\"Pix2PixSchedule Call - Decay Factor:\", decay_factor, output_stream=sys.stdout)\n",
        "\n",
        "#             learning_rate = tf.where(cond, lr_if_true, lr_if_false)\n",
        "#             # --- Debug Print ---\n",
        "#             tf.print(\"Pix2PixSchedule Call - Returned LR:\", learning_rate, output_stream=sys.stdout)\n",
        "#             return learning_rate\n",
        "\n",
        "#     def get_config(self):\n",
        "#         \"\"\"Returns the configuration of the schedule.\"\"\"\n",
        "#         return {\n",
        "#             \"initial_learning_rate\": float(self.initial_learning_rate.numpy()),\n",
        "#             \"decay_start_step\": int(self.decay_start_step.numpy()),\n",
        "#             \"decay_steps\": int(self.decay_steps.numpy()),\n",
        "#             \"name\": self.name\n",
        "#         }\n",
        "\n",
        "# --- Re-instantiate the schedule AFTER defining the modified class ---\n",
        "# logger.info(\"Re-instantiating schedule with debug prints...\")\n",
        "# lr_schedule = Pix2PixSchedule(initial_learning_rate, decay_start_step, total_decay_steps)\n",
        "\n",
        "# --- IMPORTANT: Re-instantiate the optimizers to use the new schedule instance ---\n",
        "# logger.info(\"Re-defining optimizers with the new debug schedule instance...\")\n",
        "# try:\n",
        "#     generator_optimizer = tf.keras.optimizers.Adam(\n",
        "#         learning_rate=lr_schedule, beta_1=BETA_1, beta_2=BETA_2, name='generator_adam'\n",
        "#     )\n",
        "#     discriminator_optimizer = tf.keras.optimizers.Adam(\n",
        "#         learning_rate=lr_schedule, beta_1=BETA_1, beta_2=BETA_2, name='discriminator_adam'\n",
        "#     )\n",
        "#     logger.info(\"Optimizers re-defined with debug schedule.\")\n",
        "# except Exception as e:\n",
        "#     logger.error(f\"Error re-defining optimizers with schedule: {e}\", exc_info=True)\n",
        "#     raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWU0qlALdRbf"
      },
      "outputs": [],
      "source": [
        "# # Instantiate the custom learning rate schedule\n",
        "# lr_schedule = Pix2PixSchedule(initial_learning_rate, decay_start_step, total_decay_steps)\n",
        "# logger.info(\"Pix2PixSchedule instantiated.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6kCgIwXLcGs"
      },
      "source": [
        "# Optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zHrEZ6ELwqk"
      },
      "source": [
        "### Explanation:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oTNY-99Lz2O"
      },
      "source": [
        "\n",
        "1. **Optimizer Choice:** We are using the Adam optimizer (`tf.keras.optimizers.Adam`) for both the Generator and the Discriminator. This is a standard and effective choice for GANs, including the original pix2pix implementation, known for its adaptive learning rate capabilities.\n",
        "2. **Separate Optimizers:** It's crucial to define two separate instances of the Adam optimizer (`generator_optimizer` and `discriminator_optimizer`). The Generator and Discriminator are distinct networks with their own sets of trainable parameters and are trained based on different loss functions (they have competing objectives). Therefore, they need independent optimizers to track their respective gradients and update their weights correctly.\n",
        "3. Hyperparameters:\n",
        "- `LEARNING_RATE = 0.0002`: This is a commonly used starting learning rate for Adam in GAN training and often provides a good balance between convergence speed and stability.\n",
        "- `BETA_1 = 0.5`: Adam uses beta parameters to control the exponential decay rates for its moving averages of gradients (moments). While the default for `beta_1` is often `0.9`, setting it to `0.5` is a common practice specifically for GANs. A lower `beta_1` gives less weight to past gradients (less momentum), which can help stabilize the sometimes oscillatory training dynamics between the generator and discriminator.\n",
        "- `BETA_2 = 0.999`: The default value for the second moment estimate decay rate is typically used.\n",
        "4. **Instantiation:** The code creates the two optimizer instances by calling `tf.keras.optimizers.Adam(...)` and passing the specified hyperparameters.\n",
        "5. **`AdamW` Alternative**: `AdamW` is a potential alternative (available in newer TensorFlow versions or TensorFlow Addons). `AdamW` implements weight decay more effectively than standard Adam combined with L2 regularization and is often preferred if we add significant weight decay later for regularization. For this baseline without explicit weight decay added here, standard Adam is fine.\n",
        "6. **Logging and Error Handling**: An informational message is logged confirming the optimizer configuration. The `try...except` block ensures that any errors during optimizer instantiation are caught, logged with details (`exc_info=True`), and execution is stopped (`raise`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTdQ3GG1L0G4"
      },
      "source": [
        "### Code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyYK5yipLEyG"
      },
      "outputs": [],
      "source": [
        "logger.info(\"Defining optimizers with custom LR schedule and loss scaling to enable AMP...\")\n",
        "\n",
        "try:\n",
        "    # Create the base Adam optimizers (using schedule or fixed LR)\n",
        "    # base_generator_optimizer = tf.keras.optimizers.Adam(\n",
        "    #     learning_rate=LEARNING_RATE, # Or your lr_schedule object\n",
        "    #     beta_1=BETA_1, beta_2=BETA_2, name='base_generator_adam'\n",
        "    # )\n",
        "    # Use two separate Adam optimizers: one for the Generator and one for the Discriminator\n",
        "    # generator_optimizer = mixed_precision.LossScaleOptimizer(\n",
        "    #     base_generator_optimizer, name='amp_generator_adam'\n",
        "    # )\n",
        "    # base_discriminator_optimizer = tf.keras.optimizers.Adam(\n",
        "    #     learning_rate=LEARNING_RATE, # Or schedule, possibly different rate\n",
        "    #     beta_1=BETA_1, beta_2=BETA_2, name='base_discriminator_adam'\n",
        "    # )\n",
        "    # discriminator_optimizer = mixed_precision.LossScaleOptimizer(\n",
        "    #     base_discriminator_optimizer, name='amp_discriminator_adam'\n",
        "    # )\n",
        "    # logger.info(\"Optimizers wrapped with LossScaleOptimizer for AMP.\")\n",
        "    generator_optimizer = tf.keras.optimizers.Adam(\n",
        "        learning_rate=LEARNING_RATE, # Or your lr_schedule object\n",
        "        beta_1=BETA_1, beta_2=BETA_2, name='base_generator_adam'\n",
        "    )\n",
        "    discriminator_optimizer = tf.keras.optimizers.Adam(\n",
        "        learning_rate=LEARNING_RATE, # Or schedule, possibly different rate\n",
        "        beta_1=BETA_1, beta_2=BETA_2, name='base_discriminator_adam'\n",
        "    )\n",
        "    logger.info(f\"Optimizers defined: Adam with INITIAL LR={LEARNING_RATE}, beta_1={BETA_1}, beta_2={BETA_2}\")\n",
        "    # Note: Consider AdamW later if adding significant weight decay.\n",
        "    # e.g., use tf.keras.optimizers.AdamW(...) if using TF 2.11+ or tfa.optimizers.AdamW otherwise\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error defining/wrapping optimizers: {e}\", exc_info=True)\n",
        "    raise # Stop execution if optimizers can't be created"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mid2LFGpugjn"
      },
      "source": [
        "# Metrics:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrCqPN5nuvl8"
      },
      "source": [
        "## VGG19 Perceptual loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxcgAoTauidl"
      },
      "outputs": [],
      "source": [
        "# --- Build the Feature Extractor ---\n",
        "def build_vgg_feature_extractor(layer_names, input_shape=(None, None, 3)):\n",
        "    \"\"\"Builds a VGG19 model for feature extraction.\"\"\"\n",
        "    # Ensure weights are downloaded if not present; might need internet access first time\n",
        "    try:\n",
        "        vgg = vgg19.VGG19(include_top=False, weights='imagenet', input_shape=input_shape)\n",
        "        vgg.trainable = False # Essential for metric/loss use\n",
        "        outputs = [vgg.get_layer(name).output for name in layer_names]\n",
        "        model = tf.keras.Model(vgg.input, outputs, name='vgg_feature_extractor')\n",
        "        logger.info(f\"Built VGG19 feature extractor with layers: {layer_names}\")\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to build VGG feature extractor: {e}\", exc_info=True)\n",
        "        logger.error(\"Ensure internet connection for downloading VGG weights if needed.\")\n",
        "        raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CczqQzIWv4gE"
      },
      "outputs": [],
      "source": [
        "class PerceptualMetric(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name='perceptual_metric',\n",
        "                 layer_names=METRIC_LAYER_NAMES,\n",
        "                 input_shape=(256, 256, 3), # MUST Match your drawing H, W\n",
        "                 distance_metric='mse', **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.distance_metric = distance_metric\n",
        "        # Build the feature extractor within the metric instance\n",
        "        self.feature_extractor = build_vgg_feature_extractor(layer_names, input_shape)\n",
        "        # Ensure it's not trainable (should be handled by build_vgg..., but double-check)\n",
        "        self.feature_extractor.trainable = False\n",
        "\n",
        "        # Internal state using add_weight for proper TF metric handling\n",
        "        self.total_distance = self.add_weight(name='total_distance', initializer='zeros')\n",
        "        self.count = self.add_weight(name='count', initializer='zeros', dtype=tf.float32) # Ensure count is float for division\n",
        "        logger.info(f\"PerceptualMetric '{name}' initialized with shape {input_shape}, layers: {layer_names}, metric: {distance_metric}\")\n",
        "\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            y_true: Ground truth drawings (batch, H, W, C), range [0, 1]\n",
        "            y_pred: Generated drawings (batch, H, W, C), range [0, 1]\n",
        "        \"\"\"\n",
        "        gt_drawing = tf.cast(y_true, tf.float32)\n",
        "        generated_drawing = tf.cast(y_pred, tf.float32)\n",
        "        batch_size = tf.cast(tf.shape(gt_drawing)[0], tf.float32) # Cast batch size to float\n",
        "\n",
        "        # --- Preprocessing ---\n",
        "        # 1. Ensure 3 channels\n",
        "        if generated_drawing.shape[-1] == 1:\n",
        "            generated_drawing = tf.image.grayscale_to_rgb(generated_drawing)\n",
        "        if gt_drawing.shape[-1] == 1:\n",
        "            gt_drawing = tf.image.grayscale_to_rgb(gt_drawing)\n",
        "\n",
        "        # 2. Apply VGG preprocessing (expects [0, 255] input range)\n",
        "        # Ensure input range is correct before multiplying by 255!\n",
        "        # Add clipping just in case values slightly exceed [0, 1] after denormalization\n",
        "        generated_drawing_0_1 = tf.clip_by_value(generated_drawing, 0.0, 1.0)\n",
        "        gt_drawing_0_1 = tf.clip_by_value(gt_drawing, 0.0, 1.0)\n",
        "\n",
        "        gen_prep = vgg19.preprocess_input(generated_drawing_0_1 * 255.0)\n",
        "        gt_prep = vgg19.preprocess_input(gt_drawing_0_1 * 255.0)\n",
        "\n",
        "        # --- Feature Extraction ---\n",
        "        # Use training=False explicitly\n",
        "        gen_features = self.feature_extractor(gen_prep, training=False)\n",
        "        gt_features = self.feature_extractor(gt_prep, training=False)\n",
        "\n",
        "        # Handle case where only one layer is extracted (output is not a list)\n",
        "        if not isinstance(gen_features, list):\n",
        "            gen_features = [gen_features]\n",
        "            gt_features = [gt_features]\n",
        "\n",
        "        # --- Calculate Distance ---\n",
        "        batch_distance = tf.constant(0.0, dtype=tf.float32) # Initialize with float32\n",
        "        num_layers = len(gen_features)\n",
        "        if num_layers == 0:\n",
        "            # Avoid division by zero if no layers were somehow selected\n",
        "            # This case should ideally be caught during init, but good practice\n",
        "            return\n",
        "\n",
        "        for i in range(num_layers):\n",
        "            # Calculate distance per layer and ensure it's float32\n",
        "            if self.distance_metric == 'l1':\n",
        "                layer_dist = tf.reduce_mean(tf.abs(gen_features[i] - gt_features[i]))\n",
        "            else:  # mse\n",
        "                layer_dist = tf.reduce_mean(tf.square(gen_features[i] - gt_features[i]))\n",
        "            batch_distance += tf.cast(layer_dist, tf.float32) # Accumulate as float32\n",
        "\n",
        "        # Average distance across the layers for the batch\n",
        "        avg_batch_distance = batch_distance / tf.cast(num_layers, tf.float32)\n",
        "\n",
        "        # Update state: add sum of distances for the batch, weighted by batch size\n",
        "        # This ensures batches of different sizes are weighted correctly in the final average\n",
        "        self.total_distance.assign_add(avg_batch_distance * batch_size)\n",
        "        self.count.assign_add(batch_size)\n",
        "\n",
        "\n",
        "    def result(self):\n",
        "        # Return the mean distance over all samples\n",
        "        # Avoid division by zero if count is zero\n",
        "        return tf.math.divide_no_nan(self.total_distance, self.count)\n",
        "\n",
        "    def reset_state(self):\n",
        "        # Reset weights managed by add_weight\n",
        "        self.total_distance.assign(0.0)\n",
        "        self.count.assign(0.0)\n",
        "\n",
        "    def get_config(self):\n",
        "        # Required for saving/loading models with custom metrics\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            'layer_names': self.feature_extractor.output_names, # Or store layer_names in init\n",
        "            'input_shape': self.feature_extractor.input_shape[1:], # Exclude batch dim\n",
        "            'distance_metric': self.distance_metric\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        # Required for loading models with custom metrics\n",
        "        # Note: This assumes build_vgg_feature_extractor is available in the scope\n",
        "        # Layer names might need careful handling depending on how get_config stores them\n",
        "        # Simple approach: Re-pass the config args\n",
        "        return cls(**config)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRDr55NwI5FD"
      },
      "source": [
        "# Model Instantiation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5uEwlhKI4RG"
      },
      "outputs": [],
      "source": [
        "# --- Instantiate the models ---\n",
        "try:\n",
        "    generator = build_generator()\n",
        "    discriminator = build_discriminator()\n",
        "    logger.info(\"Generator and Discriminator models built successfully.\")\n",
        "\n",
        "    # --- Optional: Log Model Summaries ---\n",
        "    # Use a function to capture summary print output and log it\n",
        "    def log_model_summary(model):\n",
        "        import io\n",
        "        stream = io.StringIO()\n",
        "        # Pass the logger's info method directly to print_fn (or use a lambda)\n",
        "        model.summary(print_fn=logger.info) # Simpler way to log summary\n",
        "        # # Alternative using capture to string first:\n",
        "        # model.summary(print_fn=lambda x: stream.write(x + '\\n'))\n",
        "        # summary_string = stream.getvalue()\n",
        "        # stream.close()\n",
        "        # logger.info(f\"Start Model Summary ({model.name}):\\n{summary_string}End Model Summary ({model.name})\")\n",
        "\n",
        "    logger.info(\"--- Generator Summary ---\")\n",
        "    log_model_summary(generator)\n",
        "    logger.info(\"--- Discriminator Summary ---\")\n",
        "    log_model_summary(discriminator)\n",
        "\n",
        "    # --- Optional: Test model outputs with dummy data (Sanity Check) ---\n",
        "    logger.info(\"Testing model outputs with dummy data...\")\n",
        "    # Create a dummy batch (use tf.ones or tf.zeros for simplicity)\n",
        "    dummy_input_batch = tf.ones([BATCH_SIZE, IMG_HEIGHT, IMG_WIDTH, 3])\n",
        "    dummy_target_batch = tf.ones([BATCH_SIZE, IMG_HEIGHT, IMG_WIDTH, 3])\n",
        "\n",
        "    gen_output = generator(dummy_input_batch, training=False)\n",
        "    disc_output = discriminator([dummy_input_batch, gen_output], training=False)\n",
        "\n",
        "    logger.info(f\" Dummy Generator output batch shape: {gen_output.shape}\") # Expect (BATCH_SIZE, 256, 256, 3)\n",
        "    logger.info(f\" Dummy Discriminator output batch shape: {disc_output.shape}\") # Expect (BATCH_SIZE, 30, 30, 1)\n",
        "    min_gen = tf.reduce_min(gen_output).numpy()\n",
        "    max_gen = tf.reduce_max(gen_output).numpy()\n",
        "    min_disc = tf.reduce_min(disc_output).numpy()\n",
        "    max_disc = tf.reduce_max(disc_output).numpy()\n",
        "\n",
        "    min_gen = float(min_gen)\n",
        "    max_gen = float(max_gen)\n",
        "    min_disc = float(min_disc)\n",
        "    max_disc = float(max_disc)\n",
        "\n",
        "    # Check output range for generator (should be roughly [-1, 1] due to tanh)\n",
        "    logger.info(f\"Dummy Generator output min/max: {min_gen:.2f}/{max_gen:.2f}\")\n",
        "    # Discriminator output is logits, so range can vary\n",
        "    logger.info(f\"Dummy Discriminator output min/max: {min_disc:.2f}/{max_disc:.2f}\")\n",
        "\n",
        "    logger.info(\"Model output shape and basic range tests passed.\")\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error building or testing models: {e}\", exc_info=True)\n",
        "    # Re-raise the exception to stop execution if model building fails\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4O52TLVTKIhO"
      },
      "outputs": [],
      "source": [
        "logging.info(f\"TensorFlow version: {tf.__version__}\")\n",
        "\n",
        "# Check for GPU availability\n",
        "gpu_devices = tf.config.list_physical_devices('GPU')\n",
        "if gpu_devices:\n",
        "    logging.info(f\"GPU detected: {gpu_devices}\")\n",
        "    # Optional: Set memory growth to prevent TF from allocating all GPU memory upfront\n",
        "    try:\n",
        "        for gpu in gpu_devices:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    except RuntimeError as e:\n",
        "        # Memory growth must be set before GPUs have been initialized\n",
        "        logging.error(e)\n",
        "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "    #logging.info(len(gpu_devices), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "    # Usually computation happens on '/GPU:0'\n",
        "else:\n",
        "    logging.info(f\"No GPU found. Using CPU.\")\n",
        "\n",
        "\n",
        "# List physical devices (GPUs and CPUs)\n",
        "physical_devices = tf.config.list_physical_devices()\n",
        "print(\"All Physical Devices:\", physical_devices)\n",
        "\n",
        "# List GPUs specifically\n",
        "gpu_devices = tf.config.list_physical_devices('GPU')\n",
        "if gpu_devices:\n",
        "    print(\"GPUs Available:\", gpu_devices)\n",
        "else:\n",
        "    print(\"No GPUs found. Using CPU.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKKBpnLWJN8a"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhG8O-zPM9rN"
      },
      "source": [
        "# Checkpointing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_XWS3dA1CwG"
      },
      "outputs": [],
      "source": [
        "!ls -la \"{checkpoint_dir}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsGb3rPdM-6l"
      },
      "outputs": [],
      "source": [
        "# === CHECKPOINTING SETUP ===\n",
        "# (Ensure generator, discriminator, generator_optimizer, discriminator_optimizer exist)\n",
        "logger.info(\"Setting up checkpointing...\")\n",
        "\n",
        "# Create a Checkpoint object - include everything you need to restore training state\n",
        "epoch_counter = tf.Variable(0, trainable=False, dtype=tf.int64)\n",
        "ckpt = tf.train.Checkpoint(generator=generator,\n",
        "                           discriminator=discriminator,\n",
        "                           generator_optimizer=generator_optimizer,\n",
        "                           discriminator_optimizer=discriminator_optimizer,\n",
        "                           epoch=epoch_counter)\n",
        "                           # Add epoch counter if you want to save/restore it too:\n",
        "                           # 'epoch': tf.Variable(0)) # Requires tf.Variable definition earlier\n",
        "\n",
        "\n",
        "# Create a CheckpointManager - manages multiple checkpoints (e.g., keeps last 5)\n",
        "# CheckpointManager requires the directory, the checkpoint object, and max_to_keep\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_dir, max_to_keep=5) # Keep latest 5 checkpoints\n",
        "logger.info(\"Checkpointing setup defined.\")\n",
        "\n",
        "# --- Attempt to restore the latest checkpoint ---\n",
        "'''\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    # try:\n",
        "    #     # ckpt.restore(ckpt_manager.latest_checkpoint).expect_partial() # Use expect_partial if models might change slightly\n",
        "    #     # logger.info(f\"Restored checkpoint from {ckpt_manager.latest_checkpoint}\")\n",
        "    #     # If you saved the epoch: logger.info(f\"Resuming training from epoch {ckpt.epoch.numpy() + 1}\")\n",
        "    # except Exception as e:\n",
        "    #     logger.warning(f\"Could not restore checkpoint: {e}\", exc_info=False) # Log as warning, don't stop if restore fails\n",
        "    #     # logger.error(f\"Could not restore checkpoint: {e}\", exc_info=True) # Or log as error if needed\n",
        "else:\n",
        "    logger.info(\"No checkpoint found at {}. Initializing from scratch.\".format(checkpoint_dir))\n",
        "    start_epoch = 0\n",
        "'''\n",
        "start_epoch = 0\n",
        "logger.info(f\"** FORCING START FROM EPOCH 0 ** (Restore logic is bypassed/commented out)\")\n",
        "\n",
        "\n",
        "# Note: Saving the checkpoint (ckpt_manager.save()) will happen periodically\n",
        "#       INSIDE the main training loop (e.g., every N epochs)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5Typn9IU8MR"
      },
      "source": [
        "# Training Step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JG6mN85U2zu"
      },
      "outputs": [],
      "source": [
        "# === TRAINING STEP FUNCTION ===\n",
        "# Ensure models (generator, discriminator), loss functions (generator_loss, discriminator_loss),\n",
        "# and optimizers (generator_optimizer, discriminator_optimizer) are defined and accessible.\n",
        "# Ensure logger object exists.\n",
        "\n",
        "logger.info(\"Defining the training step function with @tf.function...\")\n",
        "\n",
        "# The @tf.function decorator compiles the Python function into a callable\n",
        "# TensorFlow graph. This typically provides a significant speedup by optimizing\n",
        "# operations and reducing Python overhead during execution, especially inside loops.\n",
        "# Note: Debugging inside a tf.function can be harder. For debugging,\n",
        "# temporarily comment out the decorator to run in \"eager mode\" or use tf.print()\n",
        "# inside the function (which works within tf.function).\n",
        "@tf.function\n",
        "def train_step(input_image, target_image, step_counter):\n",
        "    \"\"\"\n",
        "    Performs a single training step (one batch) for the pix2pix model.\n",
        "\n",
        "    This function executes the forward passes for both the generator and discriminator,\n",
        "    calculates their respective losses, computes gradients using tf.GradientTape,\n",
        "    and applies these gradients using the optimizers.\n",
        "\n",
        "    Using tf.GradientTape:\n",
        "      - The `tf.GradientTape` context automatically records operations involving\n",
        "        trainable variables performed within its scope.\n",
        "      - This recording allows TensorFlow to compute gradients using automatic\n",
        "        differentiation when `tape.gradient()` is called.\n",
        "      - We use two separate tapes (gen_tape, disc_tape) here because we need to\n",
        "        calculate gradients and update the generator and discriminator independently\n",
        "        based on their different loss functions and trainable variables. A single\n",
        "        persistent tape could also be used but is often less clear.\n",
        "\n",
        "    Args:\n",
        "        input_image: The batch of input photos (e.g., shape [BATCH_SIZE, 256, 256, 3], range [-1, 1]).\n",
        "        target_image: The batch of corresponding ground truth diagrams\n",
        "                      (e.g., shape [BATCH_SIZE, 256, 256, 3], range [-1, 1]).\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing scalar tensor losses for monitoring:\n",
        "        (disc_loss, gen_loss_total, gen_gan_loss, gen_l1_loss)\n",
        "        Returning these scalar tensors allows logging and monitoring outside\n",
        "        this compiled function.\n",
        "    \"\"\"\n",
        "    # Open two gradient tapes simultaneously to record operations for G and D.\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        input_image = tf.debugging.check_numerics(input_image, \"Input Image\")\n",
        "        target_image = tf.debugging.check_numerics(target_image, \"Target Image\")\n",
        "\n",
        "        # --- Forward Passes ---\n",
        "        # 1. Generator produces an output image based on the input photo.\n",
        "        #    `training=True` ensures layers like BatchNormalization update their\n",
        "        #    moving statistics and Dropout layers are active.\n",
        "        gen_output = generator(input_image, training=True)\n",
        "        #gen_output = tf.debugging.check_numerics(gen_output, \"Generator Output\")\n",
        "\n",
        "\n",
        "        # 2. Discriminator evaluates the real pair (input photo + real diagram).\n",
        "        #    `training=True` ensures Discriminator's BatchNorm/Dropout are active.\n",
        "        disc_real_output = discriminator([input_image, target_image], training=True)\n",
        "        #disc_real_output = tf.debugging.check_numerics(disc_real_output, \"Disc Real Output\")\n",
        "\n",
        "        # 3. Discriminator evaluates the fake pair (input photo + generated diagram).\n",
        "        #    The same input photo is used, paired with the generator's output.\n",
        "        disc_generated_output = discriminator([input_image, gen_output], training=True)\n",
        "        #disc_generated_output = tf.debugging.check_numerics(disc_generated_output, \"Disc Generated Output\")\n",
        "\n",
        "        # --- Loss Calculation ---\n",
        "        # 4. Calculate Discriminator loss using the predefined function.\n",
        "        #    Compares D's output on real vs. fake pairs to target labels (e.g., 1s and 0s for LSGAN).\n",
        "        disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
        "        #disc_loss = tf.debugging.check_numerics(disc_loss, \"Discriminator Loss\")\n",
        "\n",
        "\n",
        "        # 5. Calculate Generator loss using the predefined function.\n",
        "        #    Includes the adversarial component (how well G fooled D) and the\n",
        "        #    L1 reconstruction component (pixel difference between generated and target).\n",
        "        gen_loss_total, gen_gan_loss, gen_l1_loss, gen_edge_loss, gen_perceptual_loss = generator_loss(disc_generated_output, gen_output, target_image)\n",
        "        # gen_loss_total = tf.debugging.check_numerics(gen_loss_total, \"Generator Total Loss\")\n",
        "        # gen_gan_loss = tf.debugging.check_numerics(gen_gan_loss, \"Generator GAN Loss\")\n",
        "        # gen_l1_loss = tf.debugging.check_numerics(gen_l1_loss, \"Generator L1 Loss\")\n",
        "        # gen_edge_loss = tf.debugging.check_numerics(gen_edge_loss, \"Generator Edge Loss\")\n",
        "        # gen_perceptual_loss = tf.debugging.check_numerics(gen_perceptual_loss, \"Generator Perceptual Loss\")\n",
        "\n",
        "        # >>> 6. Calculate Discriminator Accuracy <<<\n",
        "        # Apply sigmoid to get probabilities (assuming outputs are logits)\n",
        "        real_pred_prob = tf.sigmoid(disc_real_output)\n",
        "        fake_pred_prob = tf.sigmoid(disc_generated_output)\n",
        "\n",
        "        # Threshold probabilities at 0.5 to get predicted labels (1=Real, 0=Fake)\n",
        "        real_pred_labels = tf.cast(real_pred_prob > 0.5, tf.float32)\n",
        "        fake_pred_labels = tf.cast(fake_pred_prob > 0.5, tf.float32)\n",
        "\n",
        "        # Define true labels: 1s for real, 0s for fake\n",
        "        real_true_labels = tf.ones_like(real_pred_labels)\n",
        "        fake_true_labels = tf.zeros_like(fake_pred_labels)\n",
        "\n",
        "        # Calculate accuracy for the batch by comparing predicted vs true labels\n",
        "        disc_real_acc = tf.reduce_mean(tf.cast(tf.equal(real_pred_labels, real_true_labels), tf.float32))\n",
        "        disc_fake_acc = tf.reduce_mean(tf.cast(tf.equal(fake_pred_labels, fake_true_labels), tf.float32))\n",
        "\n",
        "        # Check for numerical issues in accuracies as well\n",
        "        # disc_real_acc = tf.debugging.check_numerics(disc_real_acc, \"Discriminator Real Accuracy\")\n",
        "        # disc_fake_acc = tf.debugging.check_numerics(disc_fake_acc, \"Discriminator Fake Accuracy\")\n",
        "\n",
        "\n",
        "\n",
        "    # --- Gradient Calculation ---\n",
        "    # 6. Calculate gradients for the Generator.\n",
        "    #    Computes derivatives of the *total generator loss* with respect to *generator's* trainable variables.\n",
        "    generator_gradients = gen_tape.gradient(gen_loss_total,\n",
        "                                            generator.trainable_variables)\n",
        "    # 7. Calculate gradients for the Discriminator.\n",
        "    #    Computes derivatives of the *discriminator loss* with respect to *discriminator's* trainable variables.\n",
        "    discriminator_gradients = disc_tape.gradient(disc_loss,\n",
        "                                                 discriminator.trainable_variables)\n",
        "\n",
        "    # --- Gradient Application ---\n",
        "    # 8. Apply calculated gradients to update Generator weights.\n",
        "    #    The optimizer modifies the variables based on the gradients. zip pairs grads with vars.\n",
        "    generator_optimizer.apply_gradients(zip(generator_gradients,\n",
        "                                            generator.trainable_variables))\n",
        "    # 9. Apply calculated gradients to update Discriminator weights.\n",
        "    # CONDITIONALLY update discriminator (e.g., every 2 steps)\n",
        "    # tf.cast is needed as step_counter is int64 usually\n",
        "    if tf.cast(step_counter, tf.int32) % DISC_UPDATE == 0:\n",
        "        discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n",
        "                                                    discriminator.trainable_variables))\n",
        "        tf.print(\"Discriminator Updated - Step:\", step_counter)\n",
        "\n",
        "\n",
        "    # --- Return Values ---\n",
        "    # Return the calculated scalar losses for external logging/monitoring.\n",
        "    return disc_loss, gen_loss_total, gen_gan_loss, gen_l1_loss, gen_edge_loss, disc_real_acc, disc_fake_acc, gen_perceptual_loss\n",
        "\n",
        "logger.info(\"Training step function defined and wrapped with @tf.function.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wt9WyKiRXggT"
      },
      "source": [
        "## Test Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZYBt-YNViOL"
      },
      "outputs": [],
      "source": [
        "# --- Test the train_step function ---\n",
        "# This block tests if the train_step function executes correctly on one batch\n",
        "# of real data from the input pipeline. It verifies that:\n",
        "#   a) The function runs without crashing (model connections, math ops work).\n",
        "#   b) It returns loss values (confirming loss calculations work).\n",
        "# c) It gives a sanity check before starting the potentially long training loop.\n",
        "logger.info(\"Testing one training step function call...\")\n",
        "try:\n",
        "    # ——— Make sure no trace is running from before ———\n",
        "    tf.summary.trace_off()\n",
        "    # Fetch one batch from the already prepared train_dataset\n",
        "    # Assumes 'train_dataset' object exists and yields (input, target) tuples\n",
        "    for test_input, test_target in train_dataset.take(1):\n",
        "        logger.info(\"Fetched one batch for testing train_step.\")\n",
        "        # Call the compiled train_step function\n",
        "        # ——— Start the Profiler ———\n",
        "        tf.profiler.experimental.start(log_dir)\n",
        "        logger.info(\"Profiler started, running train_step…\")\n",
        "        disc_loss, gen_loss_total, gen_gan_loss, gen_l1_loss, gen_edge_loss, disc_real_acc, disc_fake_acc, gen_perceptual_loss = train_step(test_input, test_target, 0)\n",
        "        # ——— Stop the Profiler ———\n",
        "        tf.profiler.experimental.stop()\n",
        "        logger.info(\"Profiler stopped, trace written to %s\", log_dir)\n",
        "\n",
        "        logger.info(\"Ran one train_step successfully.\")\n",
        "        # 1) Extract Python floats from the returned tensors\n",
        "        disc_val      = float(disc_loss.numpy())\n",
        "        gen_total_val = float(gen_loss_total.numpy())\n",
        "        gen_gan_val   = float(gen_gan_loss.numpy())\n",
        "        gen_perc_val  = float(gen_perceptual_loss.numpy())\n",
        "        l1_val        = float(gen_l1_loss.numpy())\n",
        "        edge_val      = float(gen_edge_loss.numpy())\n",
        "\n",
        "        # 2) Log them with {:.4f}\n",
        "        logger.info(f\"  Test Discriminator Loss:              {disc_val:.4f}\")\n",
        "        logger.info(f\"  Test Generator Total Loss:            {gen_total_val:.4f}\")\n",
        "        logger.info(f\"  Test Generator GAN Loss:              {gen_gan_val:.4f}\")\n",
        "        logger.info(f\"  Test Generator Perceptual Loss:       {gen_perc_val:.4f}\")\n",
        "        logger.info(f\"  Test Generator L1 Loss (unweighted):  {l1_val:.4f}\")\n",
        "        logger.info(f\"  Test Generator Edge Loss (unweighted):{edge_val:.4f}\")\n",
        "\n",
        "    logger.info(\"train_step function test completed successfully.\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error testing train_step function: {e}\", exc_info=True)\n",
        "    # Depending on severity, you might want to stop execution if this test fails\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWWFGayzN7WG"
      },
      "source": [
        "# Neptune Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etveH8snRYzf"
      },
      "outputs": [],
      "source": [
        "hparams = {\n",
        "    # Data\n",
        "    'dataset_size': dataset_size, # Get this value after listing files\n",
        "    'image_height': IMG_HEIGHT,\n",
        "    'image_width': IMG_WIDTH,\n",
        "    'input_channels': INPUT_CHANNELS,\n",
        "    'target_channels': TARGET_CHANNELS,\n",
        "    'batch_size': BATCH_SIZE,\n",
        "    'normalization_range': NORMALIZATION_RANGE,\n",
        "    'augmentations': AUGMENTATIONS,\n",
        "    # Model\n",
        "    'generator_architecture': GENERATOR_ARCHITECTURE,\n",
        "    'discriminator_architecture': DISCRIMINATOR_ARCHITECTURE,\n",
        "    'generator_output_activation': GENERATOR_OUTPUT_ACTIVATION,\n",
        "    'normalization_layer': NORMALIZATION_LAYER,\n",
        "    'generator_filters_initial': GENERATOR_FILTERS_INITIAL,\n",
        "    'discriminator_filters_initial': DISCRIMINATOR_FILTERS_INITIAL,\n",
        "    # Loss\n",
        "    'adversarial_loss_type': ADVERSARIAL_LOSS_TYPE,\n",
        "    'reconstruction_loss_type': RECONSTRUCTION_LOSS_TYPE,\n",
        "    'lambda_l1': LAMBDA_L1,\n",
        "    'lambda_edge': LAMBDA_EDGE,\n",
        "    'lamda_p' : LAMBDA_P,\n",
        "    'loss_model' : LOSS_MODEL,\n",
        "    'loss_layers' : LOSS_LAYERS,\n",
        "    'metric_layers' : METRIC_LAYER_NAMES,\n",
        "    # Optimizer\n",
        "    'optimizer_type': OPTIMIZER_TYPE,\n",
        "    'learning_rate_generator': LEARNING_RATE, # Log base LR for G\n",
        "    'learning_rate_discriminator': LEARNING_RATE * DISCRIMINATOR_LR_MULTIPLIER, # Log calculated D LR\n",
        "    'discriminator_lr_multiplier': DISCRIMINATOR_LR_MULTIPLIER, # Log the multiplier\n",
        "    'beta_1': BETA_1,\n",
        "    'beta_2': BETA_2,\n",
        "    'weight_decay': WEIGHT_DECAY,\n",
        "    # Training\n",
        "    'total_epochs': TOTAL_EPOCHS,\n",
        "    'lr_schedule': LR_SCHEDULE,\n",
        "    'description': DESCRIPTION,\n",
        "    #Metric\n",
        "    'layer_names': LOSS_LAYERS\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LY4Gwpn2N8zE"
      },
      "outputs": [],
      "source": [
        "# === NEPTUNE.AI SETUP ===\n",
        "logger.info(\"Setting up Neptune.ai experiment tracking...\")\n",
        "try:\n",
        "    if neptune:\n",
        "        # Initialize a Neptune run\n",
        "        run = neptune.init_run(\n",
        "            project=NEPTUNE_PROJECT,\n",
        "            api_token=NEPTUNE_API_TOKEN,\n",
        "            name=RUN_NAME,\n",
        "            description=DESCRIPTION\n",
        "        )\n",
        "        logger.info(f\"Neptune run initialized: {run.get_url()}\")\n",
        "\n",
        "        # --- Log Hyperparameters ---\n",
        "        run['parameters'] = hparams\n",
        "        logger.info(\"Logged hyperparameters to Neptune.\")\n",
        "\n",
        "    else:\n",
        "        logger.warning(\"Neptune client not installed or import failed. Skipping Neptune initialization.\")\n",
        "        run = None # Set run to None so later logging calls can check\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error initializing Neptune: {e}\", exc_info=True)\n",
        "    run = None # Ensure run is None if init fails\n",
        "    # Decide if you want to stop execution or continue without Neptune\n",
        "    # raise # Uncomment to stop if Neptune is critical\n",
        "\n",
        "# Note: Logging metrics (losses) and images will happen periodically\n",
        "#       INSIDE your main training loop using calls like:\n",
        "#       if run: run['train/g_loss'].append(loss_value, step=global_step)\n",
        "#       if run: run['images/samples'].append(neptune.types.File.as_image(img_array))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRr4NMDUWsLr"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6fQR9QDww9B"
      },
      "outputs": [],
      "source": [
        "initial_learning_rate = LEARNING_RATE # Initial LR\n",
        "epochs = TOTAL_EPOCHS # Your total epochs (200)\n",
        "decay_start_epoch = epochs // 2\n",
        "decay_epochs = epochs - decay_start_epoch\n",
        "def get_lr(epoch):\n",
        "  \"\"\"Calculates the learning rate for a given epoch based on linear decay.\"\"\"\n",
        "  if epoch < LR_EPOCH:\n",
        "    return initial_learning_rate\n",
        "  elif epoch < decay_start_epoch:\n",
        "    return initial_learning_rate * LR_DECAY\n",
        "  else:\n",
        "    decay_factor = 1.0 - (epoch - decay_start_epoch) / decay_epochs\n",
        "    return initial_learning_rate * LR_DECAY * max(0.0, decay_factor)\n",
        "\n",
        "logger.info(f\"Using MANUAL LR schedule: Fixed at {initial_learning_rate:.6f} for {decay_start_epoch} epochs, then linear decay for {decay_epochs} epochs.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01L269ZlZeZU"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/logs/gradient_tape --reload_interval 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tfwww0YWr3S"
      },
      "outputs": [],
      "source": [
        "print(f\"CONFIRM POLICY BEFORE TRAINING: {tf.keras.mixed_precision.global_policy().name}\")\n",
        "logger.info(f\"CONFIRM POLICY BEFORE TRAINING: {tf.keras.mixed_precision.global_policy().name}\")\n",
        "\n",
        "\n",
        "# === TRAINING LOOP ===\n",
        "# This section orchestrates the main training process. It assumes the following\n",
        "# components have been defined and initialized in previous cells:\n",
        "#   - train_dataset: A tf.data.Dataset yielding (input_image, target_image) batches.\n",
        "#   - generator: The compiled tf.keras.Model for the Generator (U-Net).\n",
        "#   - discriminator: The compiled tf.keras.Model for the Discriminator (PatchGAN).\n",
        "#   - generator_optimizer: A tf.keras.optimizers.Optimizer for the generator.\n",
        "#   - discriminator_optimizer: A tf.keras.optimizers.Optimizer for the discriminator.\n",
        "#   - generator_loss: A function calculating the generator's total loss.\n",
        "#   - discriminator_loss: A function calculating the discriminator's loss.\n",
        "#   - train_step: A @tf.function decorated function performing one G/D update.\n",
        "#   - ckpt_manager: A tf.train.CheckpointManager for saving checkpoints.\n",
        "#   - run: An active Neptune run object (or None if Neptune init failed).\n",
        "#   - logger: A configured Python logger object.\n",
        "#   - BATCH_SIZE, LAMBDA_L1, LEARNING_RATE: Hyperparameters.\n",
        "#   - hparams: Dictionary of hyperparameters (used for logging and setting EPOCHS).\n",
        "#\n",
        "# The loop iterates through epochs, and within each epoch, iterates through batches\n",
        "# of data from train_dataset. It calls the train_step function for each batch,\n",
        "# accumulates losses, periodically logs progress to the console/file and Neptune,\n",
        "# generates and logs visual samples using a fixed validation batch, and saves\n",
        "# model checkpoints.\n",
        "\n",
        "# ——— PROFILER CONFIG ———\n",
        "# which epochs to profile, or toggle on/off via a flag file\n",
        "PROFILE_EPOCHS    = {0, 5, 10}\n",
        "PROFILE_FLAG_FILE = \"/tmp/enable_profiling.flag\"\n",
        "profiling_active  = False\n",
        "# ————————————————\n",
        "\n",
        "# --- Training Configuration ---\n",
        "# Get total epochs from hyperparameters logged earlier, with a default fallback\n",
        "try:\n",
        "    EPOCHS = hparams['total_epochs']\n",
        "except NameError:\n",
        "    logger.warning(\"hparams dictionary not found, using default EPOCHS=200\")\n",
        "    EPOCHS = 200\n",
        "except KeyError:\n",
        "    logger.warning(\"'total_epochs' not found in hparams, using default EPOCHS=200\")\n",
        "    EPOCHS = 200\n",
        "\n",
        "logger.info(f\"Starting training configuration for {EPOCHS} epochs...\")\n",
        "# Log key parameters for visibility at the start of training section\n",
        "logger.info(f\"  Batch Size: {BATCH_SIZE}\")\n",
        "logger.info(f\"  Lambda L1: {LAMBDA_L1}\")\n",
        "logger.info(f\"  Lambda edge: {LAMBDA_EDGE}\")\n",
        "logger.info(f\"  Lambda perceptual: {LAMBDA_P}\")\n",
        "logger.info(f\"  Initial Learning Rate: {LEARNING_RATE}\")\n",
        "logger.info(f\"  LR Schedule: {hparams.get(LR_SCHEDULE, 'unknown')}\")\n",
        "logger.info(f\"  Checkpoint Frequency: Every {CHECKPOINT_SAVE_FREQ} epochs\")\n",
        "logger.info(f\"  Image Log Frequency: Every {IMAGE_LOG_FREQ} epochs\")\n",
        "\n",
        "\n",
        "# --- Prepare Fixed Validation Data for Visualization ---\n",
        "# We take one batch from the training set before the loop starts.\n",
        "# Using the same batch each time allows for consistent visual comparison\n",
        "# of the generator's progress over epochs.\n",
        "logger.info(\"Fetching fixed validation batch for visualization...\")\n",
        "fixed_val_input, fixed_val_target = None, None # Initialize to None\n",
        "try:\n",
        "    # Taking from train_dataset means it might be shuffled differently each run.\n",
        "    # For perfect consistency, create a separate tf.data.Dataset from validation files.\n",
        "    # Simple approach for now: take one batch from the configured training pipeline.\n",
        "    # Use .unbatch().take().batch() to ensure we get exactly BATCH_SIZE examples if possible\n",
        "    # Adjust BATCH_SIZE if it's larger than the number of available validation examples\n",
        "    val_examples_to_take = min(BATCH_SIZE, 16) # Take up to 16 examples for validation vis\n",
        "    val_dataset_vis = train_dataset.unbatch().take(val_examples_to_take).batch(val_examples_to_take)\n",
        "    fixed_val_input, fixed_val_target = next(iter(val_dataset_vis))\n",
        "    logger.info(f\"Fixed validation batch shapes: Input {fixed_val_input.shape}, Target {fixed_val_target.shape}\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Could not get validation batch: {e}\", exc_info=True)\n",
        "    logger.warning(\"Proceeding without fixed validation data for image logging.\")\n",
        "\n",
        "\n",
        "# --- Learning Rate Schedule Function (Example: Fixed then Decay) ---\n",
        "# This implements the schedule described in the original pix2pix paper.\n",
        "# Note: Using tf.keras.optimizers.schedules is generally more robust and integrates\n",
        "# directly with the optimizer, but this manual approach works for the baseline.\n",
        "initial_learning_rate = LEARNING_RATE\n",
        "decay_start_epoch = EPOCHS // 2\n",
        "decay_epochs = EPOCHS - decay_start_epoch\n",
        "\n",
        "logger.info(f\"LR schedule: Fixed at {initial_learning_rate:.6f} for {decay_start_epoch} epochs, then linear decay for {decay_epochs} epochs.\")\n",
        "\n",
        "'''\n",
        "\n",
        "# Inside your validation loop/callback\n",
        "# Assume generated_drawings, target_drawings are tensors in range [-1, 1]\n",
        "# Rescale to [0, 1] for SSIM function\n",
        "generated_0_1 = (generated_drawings + 1.0) / 2.0\n",
        "target_0_1 = (target_drawings + 1.0) / 2.0\n",
        "\n",
        "current_ssim = tf.reduce_mean(tf.image.ssim(target_0_1, generated_0_1, max_val=1.0))\n",
        "# Log current_ssim (e.g., using TensorBoard tf.summary.scalar)\n",
        "print(f\"Step {step}: Validation SSIM: {current_ssim:.4f}\")\n",
        "'''\n",
        "# --- Metrics Accumulators ---\n",
        "# Using tf.keras.metrics.Mean allows easy calculation of average loss per epoch.\n",
        "logger.info(\"Initializing Keras metrics for epoch loss averaging...\")\n",
        "epoch_disc_loss_avg = tf.keras.metrics.Mean(name='epoch_disc_loss')\n",
        "epoch_gen_loss_total_avg = tf.keras.metrics.Mean(name='epoch_gen_loss_total')\n",
        "epoch_gen_gan_loss_avg = tf.keras.metrics.Mean(name='epoch_gen_gan_loss')\n",
        "epoch_gen_l1_loss_avg = tf.keras.metrics.Mean(name='epoch_gen_l1_loss')\n",
        "epoch_edge_loss_avg = tf.keras.metrics.Mean(name='epoch_edge_loss')\n",
        "epoch_perceptual_loss_avg = tf.keras.metrics.Mean(name='epoch_perceptual_loss')\n",
        "epoch_disc_real_acc_avg = tf.keras.metrics.Mean(name='epoch_disc_real_acc')\n",
        "epoch_disc_fake_acc_avg = tf.keras.metrics.Mean(name='epoch_disc_fake_acc')\n",
        "epoch_disc_overall_acc_avg = tf.keras.metrics.Mean(name='epoch_disc_overall_acc') # overall\n",
        "\n",
        "\n",
        "# Validation Metrics\n",
        "val_ssim_ms_metric = tf.keras.metrics.Mean(name='val_ssim_ms')\n",
        "val_edge_l1_metric = tf.keras.metrics.Mean(name='val_edge_l1')\n",
        "val_lap_var_metric = tf.keras.metrics.Mean(name='val_laplacian_variance')\n",
        "val_psnr_metric = tf.keras.metrics.Mean(name='val_psnr')\n",
        "val_perceptual_diff_metric = tf.keras.metrics.Mean(name='val_perceptual_diff')\n",
        "val_l1_diff_metric = tf.keras.metrics.Mean(name='val_l1_diff')\n",
        "val_edge_diff_metric = tf.keras.metrics.Mean(name='val_edge_diff')\n",
        "\n",
        "# >>> PerceptualMetric Initialization <<<\n",
        "try:\n",
        "    val_perceptual_metric = PerceptualMetric(\n",
        "        input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), # Ensure 3 channels\n",
        "        name='val_perceptual' # Distinct name\n",
        "        # Optionally change layers: layer_names=['block1_conv1', ...]\n",
        "        # Optionally change distance: distance_metric='l1'\n",
        "    )\n",
        "    # val_perceptual_diff_metric = tf.keras.metrics.Mean(name='val_perceptual_diff') # Moved to metrics section\n",
        "except Exception as metric_init_err:\n",
        "     logger.error(f\"Failed to initialize PerceptualMetric: {metric_init_err}\", exc_info=True)\n",
        "     logger.error(\"Cannot proceed without perceptual metric. Check VGG build or input shape.\")\n",
        "     # Depending on requirements, you might exit or set val_perceptual_metric to None and add checks later\n",
        "     raise metric_init_err # Stop execution if essential\n",
        "\n",
        "\n",
        "# --- Determine Starting Epoch (for resuming from checkpoint) ---\n",
        "start_epoch = 0\n",
        "# if ckpt_manager.latest_checkpoint:\n",
        "#     # Attempt to parse epoch number from the checkpoint filename (e.g., 'ckpt-10')\n",
        "#     try:\n",
        "#         start_epoch = int(os.path.basename(ckpt_manager.latest_checkpoint).split('-')[-1])\n",
        "#         logger.info(f\"Checkpoint restored. Resuming training from epoch {start_epoch + 1}\")\n",
        "#     except ValueError:\n",
        "#         logger.warning(f\"Could not parse epoch number from checkpoint file {ckpt_manager.latest_checkpoint}. Starting epoch count from 0, although weights were restored.\")\n",
        "#     # Note: A more robust method is to save the epoch as a tf.Variable within the checkpoint object:\n",
        "#     # ckpt = tf.train.Checkpoint(..., epoch=tf.Variable(0))\n",
        "#     # And restore it: start_epoch = ckpt.epoch.numpy()\n",
        "\n",
        "logger.critical(f\"FINAL CHECK: Value of start_epoch JUST BEFORE loop starts: {start_epoch}\")\n",
        "print(f\"DEBUG PRINT: Value of start_epoch JUST BEFORE loop: {start_epoch}\") # Add a print too, just in case logging has issues\n",
        "\n",
        "# -------------- MAIN TRAINING LOOP --------------\n",
        "logger.info(f\"=== Starting Training Loop from Epoch {start_epoch + 1} ===\")\n",
        "try: # Wrap the entire loop in try/finally to ensure Neptune run is stopped\n",
        "    # Iterate through each epoch\n",
        "    for epoch in range(start_epoch, EPOCHS):\n",
        "        epoch_start_time = time.time() # Record time at the start of the epoch\n",
        "\n",
        "        # — PROFILER CONTROL AT TOP OF EPOCH LOOP —\n",
        "        should_profile = (epoch in PROFILE_EPOCHS) or os.path.exists(PROFILE_FLAG_FILE)\n",
        "        if should_profile and not profiling_active:\n",
        "            tf.profiler.experimental.start(log_dir)\n",
        "            profiling_active = True\n",
        "            logger.info(f\"Profiler ON at epoch {epoch}\")\n",
        "        elif not should_profile and profiling_active:\n",
        "            try:\n",
        "                tf.profiler.experimental.stop()\n",
        "            except tf.errors.UnavailableError:\n",
        "                logger.warning(\"Profiler.stop() called but no profiler was running.\")\n",
        "            profiling_active = False\n",
        "            logger.info(f\"Profiler OFF after epoch {epoch-1}\")\n",
        "        # — END PROFILER CONTROL —\n",
        "\n",
        "        # --- Learning Rate Update ---\n",
        "        # Get the LR for the *current step* from the optimizer state\n",
        "        # Note: optimizer.iterations is the number of steps taken by *this* optimizer\n",
        "        current_step = generator_optimizer.iterations # Or discriminator_optimizer.iterations\n",
        "        #current_lr = lr_schedule(current_step)\n",
        "        # Calculate the base scheduled learning rate for the current epoch\n",
        "        base_lr_for_epoch = get_lr(epoch)\n",
        "\n",
        "        # Assign the learning rates to the optimizers\n",
        "        gen_lr = base_lr_for_epoch # Generator uses the base scheduled rate\n",
        "        disc_lr = base_lr_for_epoch * DISCRIMINATOR_LR_MULTIPLIER # Discriminator uses modified rate\n",
        "\n",
        "        generator_optimizer.learning_rate.assign(gen_lr)\n",
        "        discriminator_optimizer.learning_rate.assign(disc_lr)\n",
        "\n",
        "        # Update logging to show both rates\n",
        "        logger.info(f\"Epoch {epoch + 1}/{EPOCHS} - Set LR -> G: {gen_lr:.6f}, D: {disc_lr:.6f}\")\n",
        "        if run: # Log potentially different LRs to Neptune if desired\n",
        "             run['train/epoch_lr_generator'].append(gen_lr, step=epoch+1)\n",
        "             run['train/epoch_lr_discriminator'].append(disc_lr, step=epoch+1)\n",
        "        # === TENSORBOARD: log learning rates ===\n",
        "        with tb_writer.as_default():\n",
        "            tf.summary.scalar('LearningRate/Generator', gen_lr, step=epoch+1)\n",
        "            tf.summary.scalar('LearningRate/Discriminator', disc_lr, step=epoch+1)\n",
        "\n",
        "        # --- Reset Metrics ---\n",
        "        # Clear the stored averages from the previous epoch before starting the new one\n",
        "        epoch_disc_loss_avg.reset_state()\n",
        "        epoch_gen_loss_total_avg.reset_state()\n",
        "        epoch_gen_gan_loss_avg.reset_state()\n",
        "        epoch_gen_l1_loss_avg.reset_state()\n",
        "        epoch_edge_loss_avg.reset_state()\n",
        "        epoch_perceptual_loss_avg.reset_state()\n",
        "        val_ssim_ms_metric.reset_state()\n",
        "        val_edge_l1_metric.reset_state()\n",
        "        val_lap_var_metric.reset_state()\n",
        "        val_psnr_metric.reset_state()\n",
        "        val_edge_l1_metric.reset_state()\n",
        "        val_l1_diff_metric.reset_state()\n",
        "        val_edge_diff_metric.reset_state()\n",
        "        epoch_disc_real_acc_avg.reset_state()\n",
        "        epoch_disc_fake_acc_avg.reset_state()\n",
        "        epoch_disc_overall_acc_avg.reset_state()\n",
        "        if 'val_perceptual_metric' in locals() and val_perceptual_metric: # Check if init succeeded\n",
        "            val_perceptual_metric.reset_state()\n",
        "            val_perceptual_diff_metric.reset_state()\n",
        "\n",
        "        # --- Batch Loop (instrumented for profiling) ---\n",
        "        logger.info(f\"Iterating through dataset batches for Epoch {epoch + 1}...\")\n",
        "        train_iter = iter(train_dataset)\n",
        "        step = 0\n",
        "        while True:\n",
        "            try:\n",
        "                step_start_time = time.time()\n",
        "\n",
        "                if profiling_active:\n",
        "                    with tf.profiler.experimental.Trace('train', step_num=step, _r=1):\n",
        "                        input_image, target_image = next(train_iter)\n",
        "                        (disc_loss,\n",
        "                        gen_loss_total,\n",
        "                        gen_gan_loss,\n",
        "                        gen_l1_loss,\n",
        "                        gen_edge_loss,\n",
        "                        disc_real_acc,\n",
        "                        disc_fake_acc,\n",
        "                        gen_perceptual_loss) = train_step(\n",
        "                            input_image,\n",
        "                            target_image,\n",
        "                            generator_optimizer.iterations\n",
        "                        )\n",
        "                else:\n",
        "                    input_image, target_image = next(train_iter)\n",
        "                    (disc_loss,\n",
        "                    gen_loss_total,\n",
        "                    gen_gan_loss,\n",
        "                    gen_l1_loss,\n",
        "                    gen_edge_loss,\n",
        "                    disc_real_acc,\n",
        "                    disc_fake_acc,\n",
        "                    gen_perceptual_loss) = train_step(\n",
        "                        input_image,\n",
        "                        target_image,\n",
        "                        generator_optimizer.iterations\n",
        "                    )\n",
        "\n",
        "                # --- Update Epoch Metrics ---\n",
        "                epoch_disc_loss_avg.update_state(disc_loss)\n",
        "                epoch_gen_loss_total_avg.update_state(gen_loss_total)\n",
        "                epoch_gen_gan_loss_avg.update_state(gen_gan_loss)\n",
        "                epoch_gen_l1_loss_avg.update_state(gen_l1_loss)\n",
        "                epoch_edge_loss_avg.update_state(gen_edge_loss)\n",
        "                epoch_disc_real_acc_avg.update_state(disc_real_acc)\n",
        "                epoch_disc_fake_acc_avg.update_state(disc_fake_acc)\n",
        "                epoch_disc_overall_acc_avg.update_state((disc_real_acc + disc_fake_acc) / 2.0)\n",
        "                epoch_perceptual_loss_avg.update_state(gen_perceptual_loss)\n",
        "\n",
        "                # --- Periodic Console Logging ---\n",
        "                if (step + 1) % CONSOLE_LOG_FREQ == 0:\n",
        "                    step_time = time.time() - step_start_time\n",
        "                    logger.info(\n",
        "                        f\"  Epoch {epoch+1}, Step {step+1}: \"\n",
        "                        f\"D Loss={epoch_disc_loss_avg.result():.4f}, \"\n",
        "                        f\"G Total={epoch_gen_loss_total_avg.result():.4f} \"\n",
        "                        f\"(G‑GAN={epoch_gen_gan_loss_avg.result():.4f}, \"\n",
        "                        f\"G‑L1={epoch_gen_l1_loss_avg.result():.4f}), \"\n",
        "                        f\"D Acc Real={epoch_disc_real_acc_avg.result():.3f}, \"\n",
        "                        f\"D Acc Fake={epoch_disc_fake_acc_avg.result():.3f}, \"\n",
        "                        f\"D Acc Overall={epoch_disc_overall_acc_avg.result():.3f} \"\n",
        "                        f\"D Perceptual={epoch_perceptual_loss_avg.result():.4f} \"\n",
        "                        f\"Time/Step={step_time:.3f}s\"\n",
        "                    )\n",
        "\n",
        "                step += 1\n",
        "            except StopIteration:\n",
        "                break\n",
        "        # --- End of Batch Loop ---\n",
        "            # --- Optional: Step-wise Neptune Logging ---\n",
        "            # Logging every step can create a lot of data points in Neptune.\n",
        "            # Usually logging epoch averages is sufficient. Uncomment if needed.\n",
        "            # global_step = epoch * estimated_steps_per_epoch + step\n",
        "            # if run:\n",
        "            #     run['train/step_disc_loss'].append(disc_loss.numpy(), step=global_step)\n",
        "            #     run['train/step_gen_loss_total'].append(gen_loss_total.numpy(), step=global_step)\n",
        "\n",
        "\n",
        "        # --- End of Epoch Actions ---\n",
        "        epoch_end_time = time.time()\n",
        "        epoch_duration = epoch_end_time - epoch_start_time\n",
        "\n",
        "        # --- Log Epoch Summary (Console/File) ---\n",
        "        # Log the final average losses for the completed epoch\n",
        "        logger.info(f\"--- Epoch {epoch + 1} Summary ---\")\n",
        "        logger.info(f\"  Avg Discriminator Loss: {epoch_disc_loss_avg.result():.4f}\")\n",
        "        logger.info(f\"  Avg Generator Total Loss: {epoch_gen_loss_total_avg.result():.4f}\")\n",
        "        logger.info(f\"  Avg Generator GAN Loss: {epoch_gen_gan_loss_avg.result():.4f}\")\n",
        "        logger.info(f\"  Avg Generator L1 Loss (Unweighted): {epoch_gen_l1_loss_avg.result():.4f}\")\n",
        "        logger.info(f\"  Avg Generator Edge Loss (Unweighted): {epoch_edge_loss_avg.result():.4f}\")\n",
        "        logger.info(f\"  Avg Generator Perceptual Loss (Unweighted): {epoch_perceptual_loss_avg.result():.4f}\")\n",
        "        logger.info(f\"  Avg Discriminator Real Acc: {epoch_disc_real_acc_avg.result():.4f}\")\n",
        "        logger.info(f\"  Avg Discriminator Fake Acc: {epoch_disc_fake_acc_avg.result():.4f}\")\n",
        "        logger.info(f\"  Avg Discriminator Overall Acc: {epoch_disc_overall_acc_avg.result():.4f}\")\n",
        "        logger.info(f\"  Epoch Duration: {epoch_duration:.2f} seconds\")\n",
        "\n",
        "        # --- Log Epoch Metrics to Neptune ---\n",
        "        if run: # Check if Neptune run is active\n",
        "            try:\n",
        "                # Log the final average for the epoch. Use step=(epoch + 1) for Neptune plots.\n",
        "                run['train/epoch_disc_loss'].append(epoch_disc_loss_avg.result().numpy(), step=epoch+1)\n",
        "                run['train/epoch_gen_loss_total'].append(epoch_gen_loss_total_avg.result().numpy(), step=epoch+1)\n",
        "                run['train/epoch_gen_gan_loss'].append(epoch_gen_gan_loss_avg.result().numpy(), step=epoch+1)\n",
        "                # Log the average unweighted L1 loss\n",
        "                run['train/epoch_gen_l1_loss'].append(epoch_gen_l1_loss_avg.result().numpy(), step=epoch+1)\n",
        "                run['train/epoch_edge_loss'].append(epoch_edge_loss_avg.result().numpy(), step=epoch+1)\n",
        "                run['train/epoch_disc_real_acc'].append(epoch_disc_real_acc_avg.result().numpy(), step=epoch+1)\n",
        "                run['train/epoch_disc_fake_acc'].append(epoch_disc_fake_acc_avg.result().numpy(), step=epoch+1)\n",
        "                run['train/epoch_disc_overall_acc'].append(epoch_disc_overall_acc_avg.result().numpy(), step=epoch+1) # Optional\n",
        "                run['train/epoch_perceptual_loss'].append(epoch_perceptual_loss_avg.result().numpy(), step=epoch+1)\n",
        "\n",
        "                # Log timing info\n",
        "                run['train/epoch_duration_sec'].append(epoch_duration, step=epoch+1)\n",
        "                logger.info(\"Logged epoch metrics to Neptune.\")\n",
        "                # Log averaged val metrics for this epoch\n",
        "            except Exception as neptune_err:\n",
        "                # Log error but don't stop training if Neptune fails\n",
        "                logger.error(f\"Neptune logging failed for epoch {epoch+1} metrics: {neptune_err}\", exc_info=False)\n",
        "             # === TENSORBOARD: log all epoch‐level metrics ===\n",
        "            with tb_writer.as_default():\n",
        "                # losses\n",
        "                tf.summary.scalar('Loss/Discriminator',        epoch_disc_loss_avg.result(),        step=epoch+1)\n",
        "                tf.summary.scalar('Loss/Generator_Total',      epoch_gen_loss_total_avg.result(),   step=epoch+1)\n",
        "                tf.summary.scalar('Loss/Generator_GAN',        epoch_gen_gan_loss_avg.result(),     step=epoch+1)\n",
        "                tf.summary.scalar('Loss/Generator_L1',         epoch_gen_l1_loss_avg.result(),      step=epoch+1)\n",
        "                tf.summary.scalar('Loss/Generator_Edge',       epoch_edge_loss_avg.result(),        step=epoch+1)\n",
        "                tf.summary.scalar('Loss/Generator_Perceptual', epoch_perceptual_loss_avg.result(),  step=epoch+1)\n",
        "\n",
        "                # discriminator accuracies\n",
        "                tf.summary.scalar('Accuracy/Disc_Real',        epoch_disc_real_acc_avg.result(),    step=epoch+1)\n",
        "                tf.summary.scalar('Accuracy/Disc_Fake',        epoch_disc_fake_acc_avg.result(),    step=epoch+1)\n",
        "                tf.summary.scalar('Accuracy/Disc_Overall',     epoch_disc_overall_acc_avg.result(), step=epoch+1)\n",
        "            # === end TensorBoard logging ===\n",
        "\n",
        "        # --- Generate and Log Validation Images Periodically ---\n",
        "        if ((epoch + 1) % IMAGE_LOG_FREQ == 0 and fixed_val_input is not None) or epoch == 0:\n",
        "            logger.info(f\"--- Starting Validation Epoch {epoch+1} ---\")\n",
        "            logger.info(f\"Generating and logging validation images for epoch {epoch+1}...\")\n",
        "            # --- 1. Reset Validation Metrics ---\n",
        "            # Assuming val_ssim_metric, val_edge_l1_metric, val_lap_var_metric exist\n",
        "            val_ssim_ms_metric.reset_state()\n",
        "            val_edge_l1_metric.reset_state()\n",
        "            val_lap_var_metric.reset_state()\n",
        "            val_psnr_metric.reset_state()\n",
        "            val_l1_diff_metric.reset_state()\n",
        "            val_edge_diff_metric.reset_state()\n",
        "            if 'val_perceptual_metric' in locals() and val_perceptual_metric:\n",
        "                 val_perceptual_metric.reset_state()\n",
        "            val_perceptual_diff_metric.reset_state()\n",
        "\n",
        "            # --- 2. Iterate over the validation dataset ---\n",
        "            # Assumes 'val_dataset' exists\n",
        "            logger.info(\"Calculating metrics over validation set...\")\n",
        "            for val_step, (val_input, val_target) in enumerate(val_dataset):\n",
        "                try:\n",
        "                    prediction_batch = generator(val_input, training=False) # Prediction for this batch\n",
        "                    prediction_batch = tf.cast(prediction_batch, tf.float32)\n",
        "                    val_target       = tf.cast(val_target,       tf.float32)\n",
        "                    # --- De-normalize for metrics expecting [0, 1] ---\n",
        "                    target_dn = (val_target + 1) / 2.0 # Assumes input range is [-1, 1]\n",
        "                    pred_dn_batch = (prediction_batch + 1) / 2.0 # Assumes output range is [-1, 1]\n",
        "                    pred_dn_batch = tf.clip_by_value(pred_dn_batch, 0.0, 1.0) # Clip just in case values go slightly out of bounds\n",
        "                    target_0_1 = tf.clip_by_value(target_dn, 0.0, 1.0)\n",
        "\n",
        "                    # --- Calculate and Update Standard Validation Metrics ---\n",
        "                    batch_ssim_ms = tf.reduce_mean(tf.image.ssim_multiscale(target_dn, pred_dn_batch, max_val=1.0))\n",
        "                    batch_psnr = tf.reduce_mean(tf.image.psnr(target_0_1, pred_dn_batch, max_val=1.0))\n",
        "                    batch_edge_l1 = calculate_edge_loss(val_target, prediction_batch)\n",
        "                    # --- Corrected LapVar Calc ---\n",
        "                    pred_gray_dn_batch = tf.image.rgb_to_grayscale(pred_dn_batch)\n",
        "                    laplacian_kernel_vals = [[0, 1, 0], [1, -4, 1], [0, 1, 0]]\n",
        "                    laplacian_kernel = tf.constant(laplacian_kernel_vals, dtype=tf.float32)\n",
        "                    laplacian_kernel = tf.reshape(laplacian_kernel, [3, 3, 1, 1])\n",
        "                    laplacian_images = tf.nn.depthwise_conv2d(\n",
        "                        pred_gray_dn_batch, laplacian_kernel, strides=[1, 1, 1, 1], padding='VALID'\n",
        "                    )\n",
        "                    variance_per_image = tf.math.reduce_variance(laplacian_images, axis=[1, 2])\n",
        "                    batch_lap_var = tf.reduce_mean(variance_per_image)\n",
        "                    # --- End LapVar Calc ---\n",
        "                    # --- Update Accumulators ---\n",
        "                    val_ssim_ms_metric.update_state(batch_ssim_ms)\n",
        "                    val_edge_l1_metric.update_state(batch_edge_l1)\n",
        "                    val_lap_var_metric.update_state(batch_lap_var)\n",
        "                    val_psnr_metric.update_state(batch_psnr)\n",
        "\n",
        "\n",
        "                    if callable(calculate_edge_loss): # Check if function exists\n",
        "                        batch_edge_l1 = calculate_edge_loss(val_target, prediction_batch) # Assumes edge loss uses [-1,1] range? Adjust if needed.\n",
        "                        val_edge_l1_metric.update_state(batch_edge_l1)\n",
        "                    # >>> UPDATE Perceptual Metric <<<\n",
        "                    if 'val_perceptual_metric' in locals() and val_perceptual_metric:\n",
        "                        # Pass the [0, 1] range images to the metric\n",
        "                        val_perceptual_metric.update_state(target_dn, pred_dn_batch)\n",
        "                        # Calculate and update val_perceptual_diff_metric inside the loop\n",
        "                        # Get the results as tensors before subtraction\n",
        "                        val_perceptual_result = val_perceptual_metric.result()\n",
        "                        epoch_perceptual_loss_result = epoch_perceptual_loss_avg.result()\n",
        "                        val_perceptual_diff = tf.reduce_mean(tf.abs(val_perceptual_result - epoch_perceptual_loss_result))\n",
        "                        val_perceptual_diff_metric.update_state(val_perceptual_diff)\n",
        "                except Exception as val_batch_err:\n",
        "                    logger.error(f\"Error calculating metrics on validation batch {val_step}: {val_batch_err}\", exc_info=False)\n",
        "            # --- Get and Log Final Averaged Metrics ---\n",
        "            try:\n",
        "                final_avg_ssim_ms = val_ssim_ms_metric.result()\n",
        "                final_avg_edge_l1 = val_edge_l1_metric.result()\n",
        "                final_avg_lap_var = val_lap_var_metric.result()\n",
        "                final_avg_psnr = val_psnr_metric.result()\n",
        "                final_avg_perceptual = tf.constant(0.0) # Default value if metric failed/disabled\n",
        "                if 'val_perceptual_metric' in locals() and val_perceptual_metric:\n",
        "                    final_avg_perceptual = val_perceptual_metric.result()\n",
        "                    final_avg_perceptual_diff = val_perceptual_diff_metric.result()\n",
        "                logger.info(\"Finished calculating validation metrics.\")\n",
        "\n",
        "                # --- 4. Log Validation Metrics to Console/File ---\n",
        "                logger.info(f\"  Epoch {epoch+1} Validation Results: \"\n",
        "                            f\"SSIM_ms={final_avg_ssim_ms:.4f}, \"\n",
        "                            f\"EdgeL1={final_avg_edge_l1:.4f}, \"\n",
        "                            f\"LapVar={final_avg_lap_var:.4f}, \"\n",
        "                            f\"PSNR={final_avg_psnr:.4f}, \"\n",
        "                            # >>> Add Perceptual to Log String <<<\n",
        "                            f\"Perceptual={final_avg_perceptual:.4f},\" # Lower is better\n",
        "                            f\"EdgeL1_diff={val_edge_diff_metric.result():.4f}, \"\n",
        "                            f\"L1_diff={val_l1_diff_metric.result():.4f}, \"\n",
        "                            f\"Perceptual_diff={val_perceptual_diff_metric.result():.4f}\"\n",
        "                            )\n",
        "\n",
        "\n",
        "                # --- 5. Log Validation Metrics to Neptune ---\n",
        "                # !!! THIS MUST ALSO BE INDENTED INSIDE THE if BLOCK !!!\n",
        "                if run and neptune:\n",
        "                    try:\n",
        "                        # Use the final average variables defined above\n",
        "                        run[f'val/epoch_ssim_ms'].append(final_avg_ssim_ms.numpy(), step=epoch+1)\n",
        "                        run[f'val/epoch_edge_l1'].append(final_avg_edge_l1.numpy(), step=epoch+1)\n",
        "                        run[f'val/epoch_laplacian_variance'].append(final_avg_lap_var.numpy(), step=epoch+1)\n",
        "                        run[f'val/epoch_psnr'].append(final_avg_psnr.numpy(), step=epoch+1)\n",
        "                        run[f'val/epoch_edge_l1_diff'].append(val_edge_diff_metric.result().numpy(), step=epoch+1)\n",
        "                        run[f'val/epoch_l1_diff'].append(val_l1_diff_metric.result().numpy(), step=epoch+1)\n",
        "                        run[f'val/epoch_perceptual_diff'].append(val_perceptual_diff_metric.result().numpy(), step=epoch+1)\n",
        "                        # >>> Log Perceptual Metric <<<\n",
        "                        # Check if metric was initialized and calculate result before logging\n",
        "                        # Ensure 'val_perceptual_metric' exists and is not\n",
        "                        if 'val_perceptual_metric' in locals() and val_perceptual_metric:\n",
        "                             run[f'val/epoch_perceptual'].append(final_avg_perceptual.numpy(), step=epoch+1)\n",
        "                        logger.info(f\"Logged validation metrics to Neptune for epoch {epoch+1}.\")\n",
        "                    except Exception as neptune_val_metric_err:\n",
        "                        logger.error(f\"Neptune VAL metric logging failed: {neptune_val_metric_err}\", exc_info=False)\n",
        "                else:\n",
        "                    logger.info(\"Neptune run not active, skipping validation metric logging.\")\n",
        "            except Exception as metric_err:\n",
        "                logger.error(f\"Error finalizing/logging validation metrics: {metric_err}\")\n",
        "\n",
        "            # Prepare images for display/logging (de-normalize, maybe concatenate)\n",
        "            prediction_fixed = generator(fixed_val_input, training=False) # Generate prediction for fixed batch\n",
        "            num_display = min(fixed_val_input.shape[0], 6) # Show up to 4 pairs\n",
        "            display_list = []\n",
        "            for i in range(num_display):\n",
        "                # De-normalize pixel values from [-1, 1] to [0, 1] range for visualization\n",
        "                input_dn  = tf.cast((fixed_val_input[i]  + 1) / 2.0, tf.float32)\n",
        "                target_dn = tf.cast((fixed_val_target[i] + 1) / 2.0, tf.float32)\n",
        "                pred_dn   = tf.cast((prediction_fixed[i]  + 1) / 2.0, tf.float32)\n",
        "\n",
        "                # Concatenate images horizontally: Input Photo | Target Diagram | Generated Diagram\n",
        "                concatenated_img = tf.concat([input_dn, target_dn, pred_dn], axis=1)\n",
        "                # Clip values to [0, 1] to prevent potential minor floating point issues during display\n",
        "                concatenated_img = tf.clip_by_value(concatenated_img, 0.0, 1.0)\n",
        "                # Convert tensor to numpy array for Neptune logging / matplotlib display\n",
        "                display_list.append(concatenated_img.numpy())\n",
        "\n",
        "            # Stack the horizontal images vertically if displaying multiple\n",
        "            if display_list:\n",
        "                  combined_display_image = np.vstack(display_list)\n",
        "                  # === TENSORBOARD: log validation images ===\n",
        "                  # stack your list into a batch: shape [N, H, W, 3]\n",
        "                  val_batch = np.stack(display_list, axis=0)\n",
        "                  with tb_writer.as_default():\n",
        "                      tf.summary.image(\n",
        "                          'Val/Input|Target|Pred',\n",
        "                          val_batch,\n",
        "                          step=epoch+1,\n",
        "                          max_outputs=val_batch.shape[0]\n",
        "                      )\n",
        "                  # === END ===\n",
        "                  # Log the combined image to Neptune\n",
        "                  if run and neptune: # Check Neptune run and import again just in case\n",
        "                      try:\n",
        "                          logger.info(f\"Attempting neptune.types.File.as_image for epoch {epoch+1}\")\n",
        "                          logger.info(f\"Image array shape: {combined_display_image.shape}, dtype: {combined_display_image.dtype}\")\n",
        "\n",
        "                          # Use neptune.types.File.as_image() to log numpy array as image\n",
        "                          neptune_img = neptune.types.File.as_image(combined_display_image)\n",
        "                          # Log under 'images/validation_samples', associate with epoch number\n",
        "                          logger.info(f\"Attempting run['images/validation_samples'].append for epoch {epoch+1}\")\n",
        "                          run[f'images/validation_samples'].append(neptune_img, step=epoch+1)\n",
        "                          logger.info(f\"Logged validation image sample to Neptune for epoch {epoch+1}.\")\n",
        "                      except Exception as neptune_img_err:\n",
        "                          logger.error(f\"Neptune image logging failed epoch {epoch+1}: {neptune_img_err}\", exc_info=True)\n",
        "\n",
        "                  # Optional: Display the same image in Colab output\n",
        "                  # plt.figure(figsize=(9, 3 * num_display)) # Adjust size based on num_display\n",
        "                  # plt.imshow(combined_display_image)\n",
        "                  # plt.axis('off')\n",
        "                  # plt.title(f\"Validation Samples Epoch {epoch+1}\\nInput | Target | Prediction\")\n",
        "                  # plt.show()\n",
        "\n",
        "\n",
        "        # --- Save Checkpoint Periodically ---\n",
        "        if (epoch + 1) % CHECKPOINT_SAVE_FREQ == 0:\n",
        "            try:\n",
        "                # Use the CheckpointManager to save the current state\n",
        "                save_path = ckpt_manager.save()\n",
        "                # If saving epoch: ckpt.epoch.assign(epoch + 1) # Update epoch variable *before* saving if included in ckpt\n",
        "                logger.info(f\"Saved checkpoint for epoch {epoch + 1} to {save_path}\")\n",
        "            except Exception as ckpt_err:\n",
        "                logger.error(f\"Failed to save checkpoint for epoch {epoch+1}: {ckpt_err}\", exc_info=True)\n",
        "                # --- Increment Epoch Counter (Using tf.Variable Method) ---\n",
        "        # Increment AFTER all processing and saving for the current epoch `epoch` is done.\n",
        "        # The variable now holds the count of completed epochs (0-based index + 1).\n",
        "        # Check if 'epoch_counter' tf.Variable exists (if implementing this method)\n",
        "        if 'epoch_counter' in locals() and isinstance(epoch_counter, tf.Variable):\n",
        "           epoch_counter.assign_add(1)\n",
        "           logger.debug(f\"Incremented epoch counter tf.Variable to {epoch_counter.numpy()}\")\n",
        "        # --- End of Current Epoch Iteration ---\n",
        "\n",
        "\n",
        "    logger.info(f\"=== Training Loop Completed after {EPOCHS} epochs ===\")\n",
        "\n",
        "\n",
        "# Ensure Neptune run is stopped cleanly even if errors occur\n",
        "finally:\n",
        "    # === STOP PROFILER if still active ===\n",
        "    if profiling_active:\n",
        "      try:\n",
        "          tf.profiler.experimental.stop()\n",
        "          logger.info(\"Profiler 🔍 OFF (final stop)\")\n",
        "      except tf.errors.UnavailableError:\n",
        "          logger.warning(\"Final Profiler.stop() called but no profiler was running.\")\n",
        "\n",
        "    # === STOP NEPTUNE RUN ===\n",
        "    logger.info(\"Attempting to stop Neptune run if active...\")\n",
        "    # Check if 'run' variable exists and is a Neptune Run object before stopping\n",
        "    if 'run' in locals() and run and isinstance(run, neptune.Run):\n",
        "        run.stop()\n",
        "        logger.info(\"Neptune run stopped.\")\n",
        "    else:\n",
        "        logger.info(\"No active Neptune run found to stop.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tpUfYZ_QY5Y"
      },
      "source": [
        "# Stop Neptune Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKPbXLg2yptb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "750hXtYIQZ6Y"
      },
      "outputs": [],
      "source": [
        "# === STOP NEPTUNE RUN ===\n",
        "# Place this in the very last cell of notebook or within a finally block\n",
        "logger.info(\"Attempting to stop Neptune run if active...\")\n",
        "if run and isinstance(run, neptune.Run): # Check if 'run' exists and is a Neptune Run object\n",
        "    run.stop()\n",
        "    logger.info(\"Neptune run stopped.\")\n",
        "else:\n",
        "    logger.info(\"No active Neptune run to stop.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqR_IxRC0NaT"
      },
      "outputs": [],
      "source": [
        "!ls -la \"{checkpoint_dir}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLGDYJFZ0N6F"
      },
      "outputs": [],
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": [],
      "private_outputs": true,
      "mount_file_id": "1X0_Df3b3Kxp_dMM5TfO-UEOGsg3LlVo9",
      "authorship_tag": "ABX9TyNijwM0Hf1cyuXj4uFFZuTg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
